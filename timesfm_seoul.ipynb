{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a99319",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "data_dir = \"data/AirPollutionSeoul\"\n",
    "measurement_files = glob.glob(os.path.join(data_dir, \"Measurement*.csv\"))\n",
    "\n",
    "all_data = []\n",
    "for file in measurement_files:\n",
    "    station_data = pd.read_csv(file)\n",
    "    station_name = os.path.basename(file).replace(\"Measurement_\", \"\").replace(\".csv\", \"\")\n",
    "    station_data[\"Station\"] = station_name\n",
    "    all_data.append(station_data)\n",
    "\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "df['Measurement date'] = pd.to_datetime(df['Measurement date'])\n",
    "df = df.sort_values(by='Measurement date')\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "pollutants = ['PM10', 'PM2.5', 'NO2', 'CO', 'SO2', 'O3']\n",
    "features = ['Measurement date', 'Station'] + pollutants\n",
    "df = df[features]\n",
    "\n",
    "df['hour'] = df['Measurement date'].dt.hour\n",
    "df['day'] = df['Measurement date'].dt.day\n",
    "df['month'] = df['Measurement date'].dt.month\n",
    "df['year'] = df['Measurement date'].dt.year\n",
    "df['dayofweek'] = df['Measurement date'].dt.dayofweek\n",
    "\n",
    "seq_len = 24\n",
    "pred_len = 12\n",
    "target_cols = pollutants\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=['Station'])\n",
    "cols_to_scale = [col for col in df_encoded.columns if col != 'Measurement date']\n",
    "scaler = StandardScaler()\n",
    "df_encoded[cols_to_scale] = scaler.fit_transform(df_encoded[cols_to_scale])\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len, pred_len, target_cols):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.target_cols = target_cols\n",
    "        self.target_indices = [data.columns.get_loc(col) for col in target_cols]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - self.pred_len + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_start = idx\n",
    "        x_end = idx + self.seq_len\n",
    "        x = self.data.iloc[x_start:x_end].values\n",
    "        \n",
    "        y_start = x_end\n",
    "        y_end = y_start + self.pred_len\n",
    "        y = self.data.iloc[y_start:y_end, self.target_indices].values\n",
    "        \n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "data_array = df_encoded.drop(columns=['Measurement date'])\n",
    "\n",
    "# Use the entire dataset instead of sampling\n",
    "train_data, temp_data = train_test_split(data_array, test_size=0.3, shuffle=False)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data, seq_len, pred_len, target_cols)\n",
    "val_dataset = TimeSeriesDataset(val_data, seq_len, pred_len, target_cols)\n",
    "test_dataset = TimeSeriesDataset(test_data, seq_len, pred_len, target_cols)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "class TimeSeriesForecaster(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, seq_len, pred_len):\n",
    "        super(TimeSeriesForecaster, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.1,\n",
    "            bidirectional=True  # Bidirectional LSTM for better feature extraction\n",
    "        )\n",
    "        \n",
    "        # Account for bidirectional LSTM (hidden_dim * 2)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, pred_len * output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)  # Add layer normalization\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.input_embedding(x)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Use the last time step from both directions\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.layer_norm(x)  # Apply layer normalization\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = x.reshape(batch_size, self.pred_len, self.output_dim)\n",
    "        return output\n",
    "\n",
    "input_dim = data_array.shape[1]\n",
    "hidden_dim = 128  # Increased hidden dim for larger dataset\n",
    "output_dim = len(target_cols)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = TimeSeriesForecaster(input_dim, hidden_dim, output_dim, seq_len, pred_len).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Learning rate scheduler for better convergence\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "epochs = 40  # Increased epochs for the full dataset\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_iter):\n",
    "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_iter.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        # Optional: gradient accumulation for large datasets\n",
    "        # if batch_idx % 4 == 0:  # Accumulate gradients for 4 batches\n",
    "        #     optimizer.step()\n",
    "        #     optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_iter = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Valid]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_iter):\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_iter.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Use learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_seoul_model.pt\")\n",
    "        print(f\"Saved model with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(range(1, epochs + 1), val_losses, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('loss_curves.png')\n",
    "plt.show()\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_seoul_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_inputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        all_preds.append(output.cpu().numpy())\n",
    "        all_targets.append(target.cpu().numpy())\n",
    "        all_inputs.append(data.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "all_inputs = np.concatenate(all_inputs, axis=0)\n",
    "\n",
    "metrics_per_hour = []\n",
    "for hour in range(pred_len):\n",
    "    hour_metrics = []\n",
    "    for i, pollutant in enumerate(target_cols):\n",
    "        mae = mean_absolute_error(all_targets[:, hour, i], all_preds[:, hour, i])\n",
    "        rmse = np.sqrt(mean_squared_error(all_targets[:, hour, i], all_preds[:, hour, i]))\n",
    "        r2 = r2_score(all_targets[:, hour, i], all_preds[:, hour, i])\n",
    "        hour_metrics.append({\n",
    "            'Hour': hour + 1,\n",
    "            'Pollutant': pollutant,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2\n",
    "        })\n",
    "    metrics_per_hour.extend(hour_metrics)\n",
    "\n",
    "hourly_metrics_df = pd.DataFrame(metrics_per_hour)\n",
    "print(\"\\nHourly Prediction Metrics:\")\n",
    "print(hourly_metrics_df.head(12))\n",
    "\n",
    "pivot_df = hourly_metrics_df.pivot_table(index='Hour', columns='Pollutant', values='RMSE')\n",
    "plt.figure(figsize=(14, 8))\n",
    "for pollutant in pivot_df.columns:\n",
    "    plt.plot(pivot_df.index, pivot_df[pollutant], marker='o', label=pollutant)\n",
    "plt.xlabel('Prediction Hour')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE by Hour for Each Pollutant')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, pred_len + 1))\n",
    "plt.savefig('rmse_by_hour.png')\n",
    "plt.show()\n",
    "\n",
    "num_samples = 5\n",
    "fig, axes = plt.subplots(len(target_cols), num_samples, figsize=(20, 15), sharex=True)\n",
    "\n",
    "for i, pollutant in enumerate(target_cols):\n",
    "    sample_indices = np.random.choice(len(all_preds), num_samples, replace=False)\n",
    "    \n",
    "    for j, idx in enumerate(sample_indices):\n",
    "        ax = axes[i, j]\n",
    "        ax.plot(range(pred_len), all_preds[idx, :, i], 'r-', linewidth=2, label='Predicted')\n",
    "        ax.plot(range(pred_len), all_targets[idx, :, i], 'b-', linewidth=2, label='Actual')\n",
    "        ax.set_title(f'{pollutant} - Sample {j+1}')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        if i == len(target_cols) - 1:\n",
    "            ax.set_xlabel('Hours Ahead')\n",
    "        \n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f'{pollutant} Level')\n",
    "            \n",
    "        if i == 0 and j == 0:\n",
    "            ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_samples.png')\n",
    "plt.show()\n",
    "\n",
    "overall_metrics = []\n",
    "for i, pollutant in enumerate(target_cols):\n",
    "    mae = mean_absolute_error(all_targets[:, :, i].flatten(), all_preds[:, :, i].flatten())\n",
    "    rmse = np.sqrt(mean_squared_error(all_targets[:, :, i].flatten(), all_preds[:, :, i].flatten()))\n",
    "    r2 = r2_score(all_targets[:, :, i].flatten(), all_preds[:, :, i].flatten())\n",
    "    \n",
    "    overall_metrics.append({\n",
    "        'Pollutant': pollutant,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(overall_metrics)\n",
    "print(\"\\nOverall Metrics by Pollutant:\")\n",
    "print(metrics_df)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "metrics_df.plot(x='Pollutant', y=['MAE', 'RMSE'], kind='bar', ax=plt.gca())\n",
    "plt.title('MAE and RMSE by Pollutant')\n",
    "plt.ylabel('Error Value')\n",
    "plt.grid(axis='y')\n",
    "plt.savefig('error_by_pollutant.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(metrics_df['Pollutant'], metrics_df['R2'])\n",
    "plt.title('R² Score by Pollutant')\n",
    "plt.ylabel('R² Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.savefig('r2_by_pollutant.png')\n",
    "plt.show()\n",
    "\n",
    "import joblib\n",
    "joblib.dump(scaler, \"seoul_air_scaler.pkl\")\n",
    "\n",
    "sample_input = next(iter(test_loader))[0][0].unsqueeze(0)\n",
    "torch.save(sample_input, \"sample_input.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566482a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
