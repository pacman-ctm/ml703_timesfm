{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dublin Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:07:03.876694Z",
     "iopub.status.busy": "2025-05-06T12:07:03.876332Z",
     "iopub.status.idle": "2025-05-06T12:13:59.475954Z",
     "shell.execute_reply": "2025-05-06T12:13:59.474289Z",
     "shell.execute_reply.started": "2025-05-06T12:07:03.876670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"muted\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/dublinair/airview_dublincity_roaddata_ugm3.csv')\n",
    "\n",
    "print(f\"Loaded data with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "pollutant_columns = [col for col in df.columns if col.endswith('_ugm3') or col.endswith('_mgm3')]\n",
    "print(f\"\\nFound {len(pollutant_columns)} pollutant columns: {pollutant_columns}\")\n",
    "\n",
    "missing_values = df[pollutant_columns].isnull().sum()\n",
    "print(\"\\nMissing values in pollutant columns:\")\n",
    "print(missing_values)\n",
    "\n",
    "def normalize_series(series, method='minmax'):\n",
    "    series_values = series.values.reshape(-1, 1)\n",
    "    if method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        return series, None\n",
    "    \n",
    "    normalized_values = scaler.fit_transform(series_values)\n",
    "    normalized_series = pd.Series(normalized_values.flatten(), index=series.index)\n",
    "    return normalized_series, scaler\n",
    "\n",
    "def run_arima_analysis(data, column_name, order=(1,1,1), test_size=0.2, normalize_method=None):\n",
    "    series = data[column_name].dropna()\n",
    "    series = pd.to_numeric(series, errors='coerce')\n",
    "    series = series.dropna()\n",
    "    \n",
    "    if len(series) < 30:\n",
    "        print(f\"Not enough data points for {column_name}. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    original_series = series.copy()\n",
    "    scaler = None\n",
    "    \n",
    "    if normalize_method:\n",
    "        series, scaler = normalize_series(series, normalize_method)\n",
    "        print(f\"Applied {normalize_method} normalization to {column_name}\")\n",
    "    \n",
    "    train_size = int(len(series) * (1 - test_size))\n",
    "    train, test = series[:train_size].values, series[train_size:].values\n",
    "    original_test = original_series.iloc[train_size:].values if normalize_method else None\n",
    "    \n",
    "    print(f\"\\nAnalyzing {column_name} with ARIMA{order}\")\n",
    "    print(f\"Training data size: {len(train)}\")\n",
    "    print(f\"Test data size: {len(test)}\")\n",
    "    \n",
    "    try:\n",
    "        model = ARIMA(train, order=order)\n",
    "        model_fit = model.fit()\n",
    "        \n",
    "        forecasts = model_fit.forecast(steps=len(test))\n",
    "        \n",
    "        if scaler:\n",
    "            forecasts_2d = forecasts.reshape(-1, 1)\n",
    "            inverse_forecasts = scaler.inverse_transform(forecasts_2d)\n",
    "            inverse_forecasts = inverse_forecasts.flatten()\n",
    "            \n",
    "            mae = mean_absolute_error(original_test, inverse_forecasts)\n",
    "            mse = mean_squared_error(original_test, inverse_forecasts)\n",
    "            rmse = np.sqrt(mse)\n",
    "            forecasts_for_plot = inverse_forecasts\n",
    "            actual_for_plot = original_test\n",
    "        else:\n",
    "            mae = mean_absolute_error(test, forecasts)\n",
    "            mse = mean_squared_error(test, forecasts)\n",
    "            rmse = np.sqrt(mse)\n",
    "            forecasts_for_plot = forecasts\n",
    "            actual_for_plot = test\n",
    "        \n",
    "        neg_log_likelihood = -model_fit.llf\n",
    "        \n",
    "        print(f\"Results for {column_name} - ARIMA{order}:\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"Negative Log-Likelihood: {neg_log_likelihood:.4f}\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(actual_for_plot, label='Actual', color='blue', marker='o', markersize=4, linestyle='-', linewidth=1)\n",
    "        plt.plot(forecasts_for_plot, label='Forecast', color='red', marker='x', markersize=4, linestyle='--', linewidth=1)\n",
    "        norm_text = f\" ({normalize_method} normalized)\" if normalize_method else \"\"\n",
    "        plt.title(f'ARIMA{order} Forecast vs Actual for {column_name}{norm_text}')\n",
    "        plt.xlabel('Test Sample Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{column_name.replace('/', '_')}_{normalize_method}_arima_forecast.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'column': column_name,\n",
    "            'order': order,\n",
    "            'normalize_method': normalize_method,\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'neg_log_likelihood': neg_log_likelihood,\n",
    "            'model': model_fit,\n",
    "            'forecasts': forecasts_for_plot,\n",
    "            'actual': actual_for_plot\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting ARIMA for {column_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "results = {}\n",
    "for column in pollutant_columns:\n",
    "    for norm_method in [None, 'minmax', 'standard']:\n",
    "        norm_key = f\"{column}_{norm_method}\" if norm_method else column\n",
    "        result = run_arima_analysis(df, column, normalize_method=norm_method)\n",
    "        if result:\n",
    "            results[norm_key] = result\n",
    "\n",
    "if results:\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Pollutant': [],\n",
    "        'Normalization': [],\n",
    "        'ARIMA_Order': [],\n",
    "        'MAE': [],\n",
    "        'MSE': [],\n",
    "        'RMSE': [],\n",
    "        'Neg_Log_Likelihood': []\n",
    "    })\n",
    "    \n",
    "    for key, result in results.items():\n",
    "        new_row = pd.DataFrame({\n",
    "            'Pollutant': [result['column']],\n",
    "            'Normalization': [result['normalize_method'] if result['normalize_method'] else 'None'],\n",
    "            'ARIMA_Order': [str(result['order'])],\n",
    "            'MAE': [result['mae']],\n",
    "            'MSE': [result['mse']],\n",
    "            'RMSE': [result['rmse']],\n",
    "            'Neg_Log_Likelihood': [result['neg_log_likelihood']]\n",
    "        })\n",
    "        metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n",
    "    \n",
    "    print(\"\\nMetrics Summary:\")\n",
    "    print(metrics_df)\n",
    "    \n",
    "    metrics_df.to_csv('arima_metrics_results.csv', index=False)\n",
    "    print(\"Results saved to 'arima_metrics_results.csv'\")\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='Pollutant', y='RMSE', hue='Normalization', data=metrics_df)\n",
    "    plt.title('RMSE by Pollutant and Normalization')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x='Pollutant', y='MAE', hue='Normalization', data=metrics_df)\n",
    "    plt.title('MAE by Pollutant and Normalization')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x='Pollutant', y='MSE', hue='Normalization', data=metrics_df)\n",
    "    plt.title('MSE by Pollutant and Normalization')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.barplot(x='Pollutant', y='Neg_Log_Likelihood', hue='Normalization', data=metrics_df)\n",
    "    plt.title('Negative Log-Likelihood by Pollutant and Normalization')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('arima_metrics_normalization_comparison.png')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No successful ARIMA models to report.\")\n",
    "\n",
    "def find_best_arima_model(data, column_name, p_range=(0,2), d_range=(0,2), q_range=(0,2), normalize_method=None):\n",
    "    series = data[column_name].dropna()\n",
    "    series = pd.to_numeric(series, errors='coerce')\n",
    "    series = series.dropna()\n",
    "    \n",
    "    if len(series) < 30:\n",
    "        print(f\"Not enough data points for {column_name}. Skipping.\")\n",
    "        return None, None\n",
    "    \n",
    "    original_series = series.copy()\n",
    "    scaler = None\n",
    "    \n",
    "    if normalize_method:\n",
    "        series, scaler = normalize_series(series, normalize_method)\n",
    "    \n",
    "    train_size = int(len(series) * 0.8)\n",
    "    train, test = series[:train_size].values, series[train_size:].values\n",
    "    original_test = original_series.iloc[train_size:].values if normalize_method else None\n",
    "    \n",
    "    print(f\"\\nFinding best ARIMA model for {column_name}\")\n",
    "    print(f\"Training data size: {len(train)}\")\n",
    "    print(f\"Test data size: {len(test)}\")\n",
    "    \n",
    "    grid_results = []\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    best_order = None\n",
    "    best_result = None\n",
    "    \n",
    "    for p in range(p_range[0], p_range[1] + 1):\n",
    "        for d in range(d_range[0], d_range[1] + 1):\n",
    "            for q in range(q_range[0], q_range[1] + 1):\n",
    "                try:\n",
    "                    order = (p, d, q)\n",
    "                    model = ARIMA(train, order=order)\n",
    "                    model_fit = model.fit()\n",
    "                    \n",
    "                    forecasts = model_fit.forecast(steps=len(test))\n",
    "                    \n",
    "                    if scaler:\n",
    "                        forecasts_2d = forecasts.reshape(-1, 1)\n",
    "                        inverse_forecasts = scaler.inverse_transform(forecasts_2d)\n",
    "                        inverse_forecasts = inverse_forecasts.flatten()\n",
    "                        \n",
    "                        mae = mean_absolute_error(original_test, inverse_forecasts)\n",
    "                        mse = mean_squared_error(original_test, inverse_forecasts)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                    else:\n",
    "                        mae = mean_absolute_error(test, forecasts)\n",
    "                        mse = mean_squared_error(test, forecasts)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                    \n",
    "                    neg_log_likelihood = -model_fit.llf\n",
    "                    \n",
    "                    print(f\"ARIMA{order} - RMSE: {rmse:.4f}, NegLogLike: {neg_log_likelihood:.4f}\")\n",
    "                    \n",
    "                    grid_results.append({\n",
    "                        'p': p,\n",
    "                        'd': d,\n",
    "                        'q': q,\n",
    "                        'RMSE': rmse,\n",
    "                        'MAE': mae,\n",
    "                        'MSE': mse,\n",
    "                        'Neg_Log_Likelihood': neg_log_likelihood\n",
    "                    })\n",
    "                    \n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_model = model_fit\n",
    "                        best_order = order\n",
    "                        best_result = {\n",
    "                            'column': column_name,\n",
    "                            'order': order,\n",
    "                            'normalize_method': normalize_method,\n",
    "                            'mae': mae,\n",
    "                            'mse': mse,\n",
    "                            'rmse': rmse,\n",
    "                            'neg_log_likelihood': neg_log_likelihood,\n",
    "                            'model': model_fit\n",
    "                        }\n",
    "                        \n",
    "                        if scaler:\n",
    "                            best_result['forecasts'] = inverse_forecasts\n",
    "                            best_result['actual'] = original_test\n",
    "                        else:\n",
    "                            best_result['forecasts'] = forecasts\n",
    "                            best_result['actual'] = test\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error fitting ARIMA{order}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    grid_df = pd.DataFrame(grid_results)\n",
    "    \n",
    "    if not grid_df.empty:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        pivot_df = grid_df.pivot_table(\n",
    "            index='p', columns=['d', 'q'], values='RMSE'\n",
    "        )\n",
    "        sns.heatmap(pivot_df, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "        norm_text = f\" ({normalize_method} normalized)\" if normalize_method else \"\"\n",
    "        plt.title(f'RMSE for Different ARIMA Parameters - {column_name}{norm_text}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{column_name.replace('/', '_')}_{normalize_method}_parameter_grid.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    if best_model:\n",
    "        print(f\"\\nBest model for {column_name}: ARIMA{best_order}\")\n",
    "        print(f\"MAE: {best_result['mae']:.4f}\")\n",
    "        print(f\"MSE: {best_result['mse']:.4f}\")\n",
    "        print(f\"RMSE: {best_result['rmse']:.4f}\")\n",
    "        print(f\"Negative Log-Likelihood: {best_result['neg_log_likelihood']:.4f}\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(best_result['actual'], 'b-o', markersize=4, label='Actual')\n",
    "        plt.plot(best_result['forecasts'], 'r--x', markersize=4, label='Forecast')\n",
    "        norm_text = f\" ({normalize_method} normalized)\" if normalize_method else \"\"\n",
    "        plt.title(f\"Best ARIMA{best_order} for {column_name}{norm_text}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{column_name.replace('/', '_')}_{normalize_method}_best_arima.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nBest model summary:\")\n",
    "        print(best_model.summary())\n",
    "        \n",
    "        return best_result, grid_df\n",
    "    else:\n",
    "        print(f\"No successful models for {column_name}\")\n",
    "        return None, None\n",
    "\n",
    "pollutant_to_optimize = 'NO2_ugm3'\n",
    "for norm_method in [None, 'minmax', 'standard']:\n",
    "    best_model_result, grid_results = find_best_arima_model(df, pollutant_to_optimize, normalize_method=norm_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seoul Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:04:15.802266Z",
     "iopub.status.busy": "2025-05-06T12:04:15.801944Z",
     "iopub.status.idle": "2025-05-06T12:07:02.864464Z",
     "shell.execute_reply": "2025-05-06T12:07:02.863393Z",
     "shell.execute_reply.started": "2025-05-06T12:04:15.802245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"muted\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "def load_seoul_data():\n",
    "    try:\n",
    "        base_path = '../input/air-pollution-in-seoul/AirPollutionSeoul'\n",
    "        \n",
    "        measurement_path = os.path.join(base_path, 'Original Data/Measurement_summary.csv')\n",
    "        if not os.path.exists(measurement_path):\n",
    "            measurement_path = os.path.join(base_path, 'Measurement_summary.csv')\n",
    "        \n",
    "        if not os.path.exists(measurement_path):\n",
    "            csv_files = glob.glob('../input/air-pollution-in-seoul/**/*.csv', recursive=True)\n",
    "            if csv_files:\n",
    "                measurement_path = csv_files[0]\n",
    "                print(f\"Using {measurement_path}\")\n",
    "        \n",
    "        df = pd.read_csv(measurement_path)\n",
    "        \n",
    "        print(f\"Loaded measurement data with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "        print(f\"Available columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Seoul data: {str(e)}\")\n",
    "        try:\n",
    "            possible_paths = glob.glob('../input/air-pollution-in-seoul/**/*.csv', recursive=True)\n",
    "            if possible_paths:\n",
    "                print(f\"Found these potential CSV files: {possible_paths}\")\n",
    "                df = pd.read_csv(possible_paths[0])\n",
    "                print(f\"Loaded {possible_paths[0]} with {df.shape[0]} rows\")\n",
    "                return df\n",
    "            else:\n",
    "                raise ValueError(\"No CSV files found\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Secondary loading attempt failed: {str(e2)}\")\n",
    "            raise ValueError(\"Failed to load Seoul air quality data\")\n",
    "\n",
    "def prepare_time_series(df, station_code, pollutant, normalize=None):\n",
    "    try:\n",
    "        filtered_df = df[df['Station code'] == station_code].copy()\n",
    "        \n",
    "        if filtered_df.empty:\n",
    "            available_stations = df['Station code'].unique()\n",
    "            print(f\"No data found for station {station_code}\")\n",
    "            print(f\"Available stations: {available_stations}\")\n",
    "            \n",
    "            station_code = available_stations[0]\n",
    "            filtered_df = df[df['Station code'] == station_code].copy()\n",
    "            print(f\"Using station {station_code} instead\")\n",
    "        \n",
    "        if pollutant not in filtered_df.columns:\n",
    "            available_pollutants = [col for col in filtered_df.columns \n",
    "                                   if col in ['SO2', 'NO2', 'O3', 'CO', 'PM10', 'PM2.5']]\n",
    "            print(f\"Pollutant '{pollutant}' not found\")\n",
    "            print(f\"Available pollutants: {available_pollutants}\")\n",
    "            \n",
    "            pollutant = available_pollutants[0]\n",
    "            print(f\"Using pollutant '{pollutant}' instead\")\n",
    "        \n",
    "        filtered_df['Timestamp'] = pd.to_datetime(filtered_df['Measurement date'])\n",
    "        filtered_df = filtered_df.sort_values('Timestamp')\n",
    "        filtered_df.set_index('Timestamp', inplace=True)\n",
    "        \n",
    "        series = filtered_df[pollutant].copy()\n",
    "        series = series.dropna()\n",
    "        \n",
    "        original_series = series.copy()\n",
    "        scaler = None\n",
    "        \n",
    "        if normalize == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "            series_values = series.values.reshape(-1, 1)\n",
    "            normalized_values = scaler.fit_transform(series_values)\n",
    "            series = pd.Series(normalized_values.flatten(), index=series.index)\n",
    "            print(f\"Applied Min-Max normalization (range: 0-1)\")\n",
    "        \n",
    "        elif normalize == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "            series_values = series.values.reshape(-1, 1)\n",
    "            normalized_values = scaler.fit_transform(series_values)\n",
    "            series = pd.Series(normalized_values.flatten(), index=series.index)\n",
    "            print(f\"Applied Standard normalization (mean=0, std=1)\")\n",
    "        \n",
    "        print(f\"Prepared time series for station {station_code}, pollutant {pollutant}\")\n",
    "        print(f\"Time series length: {len(series)}\")\n",
    "        if not series.empty:\n",
    "            print(f\"Time range: {series.index.min()} to {series.index.max()}\")\n",
    "            if normalize:\n",
    "                print(f\"Original value range: {original_series.min():.4f} to {original_series.max():.4f}\")\n",
    "                print(f\"Normalized value range: {series.min():.4f} to {series.max():.4f}\")\n",
    "            else:\n",
    "                print(f\"Value range: {series.min():.4f} to {series.max():.4f}\")\n",
    "        \n",
    "        return series, original_series, scaler, pollutant\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing time series: {str(e)}\")\n",
    "        return pd.Series([]), pd.Series([]), None, pollutant\n",
    "\n",
    "def analyze_time_series(series, station_code, pollutant):\n",
    "    if len(series) < 10:\n",
    "        print(\"Not enough data points for time series analysis\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(series)\n",
    "    plt.title(f'Time Series - Station {station_code}, {pollutant}')\n",
    "    plt.ylabel(f'{pollutant} Value')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    ax2 = plt.subplot(3, 1, 2)\n",
    "    plot_acf(series, ax=ax2, lags=40)\n",
    "    plt.title('Autocorrelation Function')\n",
    "    \n",
    "    ax3 = plt.subplot(3, 1, 3)\n",
    "    plot_pacf(series, ax=ax3, lags=40)\n",
    "    plt.title('Partial Autocorrelation Function')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Station{station_code}_{pollutant}_time_series_analysis.png\")\n",
    "    plt.show()\n",
    "\n",
    "def run_arima_analysis(series, original_series, scaler, station_code, pollutant, order=(1,1,1), test_size=0.2):\n",
    "    if len(series) < 30:\n",
    "        print(f\"Not enough data points for ARIMA analysis. Need at least 30, got {len(series)}\")\n",
    "        return None\n",
    "    \n",
    "    train_size = int(len(series) * (1 - test_size))\n",
    "    train, test = series.iloc[:train_size].values, series.iloc[train_size:].values\n",
    "    original_test = original_series.iloc[train_size:].values if original_series is not None else None\n",
    "    \n",
    "    print(f\"\\nAnalyzing Station {station_code}, {pollutant} with ARIMA{order}\")\n",
    "    print(f\"Training data size: {len(train)}\")\n",
    "    print(f\"Test data size: {len(test)}\")\n",
    "    \n",
    "    try:\n",
    "        model = ARIMA(train, order=order)\n",
    "        model_fit = model.fit()\n",
    "        \n",
    "        forecasts = model_fit.forecast(steps=len(test))\n",
    "        \n",
    "        if scaler is not None:\n",
    "            forecasts_2d = forecasts.reshape(-1, 1)\n",
    "            inverse_forecasts = scaler.inverse_transform(forecasts_2d)\n",
    "            inverse_forecasts = inverse_forecasts.flatten()\n",
    "            \n",
    "            mae = mean_absolute_error(original_test, inverse_forecasts)\n",
    "            mse = mean_squared_error(original_test, inverse_forecasts)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            print(\"Metrics calculated on original scale data\")\n",
    "        else:\n",
    "            mae = mean_absolute_error(test, forecasts)\n",
    "            mse = mean_squared_error(test, forecasts)\n",
    "            rmse = np.sqrt(mse)\n",
    "        \n",
    "        neg_log_likelihood = -model_fit.llf\n",
    "        \n",
    "        print(f\"Results for Station {station_code}, {pollutant} - ARIMA{order}:\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"Negative Log-Likelihood: {neg_log_likelihood:.4f}\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        if scaler is not None:\n",
    "            plt.plot(original_test, label='Actual', color='blue', marker='o', markersize=3, linestyle='-', linewidth=1)\n",
    "            plt.plot(inverse_forecasts, label='Forecast', color='red', marker='x', markersize=3, linestyle='--', linewidth=1)\n",
    "            plt.title(f'ARIMA{order} Forecast vs Actual for Station {station_code}, {pollutant} (Original Scale)')\n",
    "            plt.ylabel(f'{pollutant} Value')\n",
    "        else:\n",
    "            plt.plot(test, label='Actual', color='blue', marker='o', markersize=3, linestyle='-', linewidth=1)\n",
    "            plt.plot(forecasts, label='Forecast', color='red', marker='x', markersize=3, linestyle='--', linewidth=1)\n",
    "            plt.title(f'ARIMA{order} Forecast vs Actual for Station {station_code}, {pollutant}')\n",
    "            plt.ylabel(f'{pollutant} Value')\n",
    "        \n",
    "        plt.xlabel('Test Sample Index')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"Station{station_code}_{pollutant}_arima_forecast.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'station_code': station_code,\n",
    "            'pollutant': pollutant,\n",
    "            'order': order,\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'neg_log_likelihood': neg_log_likelihood,\n",
    "            'model': model_fit,\n",
    "            'forecasts': forecasts if scaler is None else inverse_forecasts,\n",
    "            'actual': test if scaler is None else original_test\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting ARIMA for Station {station_code}, {pollutant}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def find_best_arima_model(series, original_series, scaler, station_code, pollutant, p_range=(0,2), d_range=(0,2), q_range=(0,2)):\n",
    "    if len(series) < 30:\n",
    "        print(f\"Not enough data points for ARIMA parameter optimization. Need at least 30, got {len(series)}\")\n",
    "        return None, None\n",
    "    \n",
    "    train_size = int(len(series) * 0.8)\n",
    "    train, test = series.iloc[:train_size].values, series.iloc[train_size:].values\n",
    "    original_test = original_series.iloc[train_size:].values if original_series is not None else None\n",
    "    \n",
    "    print(f\"\\nFinding best ARIMA model for Station {station_code}, {pollutant}\")\n",
    "    print(f\"Training data size: {len(train)}\")\n",
    "    print(f\"Test data size: {len(test)}\")\n",
    "    \n",
    "    grid_results = []\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    best_order = None\n",
    "    best_result = None\n",
    "    \n",
    "    for p in range(p_range[0], p_range[1] + 1):\n",
    "        for d in range(d_range[0], d_range[1] + 1):\n",
    "            for q in range(q_range[0], q_range[1] + 1):\n",
    "                try:\n",
    "                    order = (p, d, q)\n",
    "                    model = ARIMA(train, order=order)\n",
    "                    model_fit = model.fit()\n",
    "                    \n",
    "                    forecasts = model_fit.forecast(steps=len(test))\n",
    "                    \n",
    "                    if scaler is not None:\n",
    "                        forecasts_2d = forecasts.reshape(-1, 1)\n",
    "                        inverse_forecasts = scaler.inverse_transform(forecasts_2d)\n",
    "                        inverse_forecasts = inverse_forecasts.flatten()\n",
    "                        \n",
    "                        mae = mean_absolute_error(original_test, inverse_forecasts)\n",
    "                        mse = mean_squared_error(original_test, inverse_forecasts)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                    else:\n",
    "                        mae = mean_absolute_error(test, forecasts)\n",
    "                        mse = mean_squared_error(test, forecasts)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                    \n",
    "                    neg_log_likelihood = -model_fit.llf\n",
    "                    \n",
    "                    print(f\"ARIMA{order} - RMSE: {rmse:.4f}, NegLogLike: {neg_log_likelihood:.4f}\")\n",
    "                    \n",
    "                    grid_results.append({\n",
    "                        'p': p,\n",
    "                        'd': d,\n",
    "                        'q': q,\n",
    "                        'RMSE': rmse,\n",
    "                        'MAE': mae,\n",
    "                        'MSE': mse,\n",
    "                        'Neg_Log_Likelihood': neg_log_likelihood\n",
    "                    })\n",
    "                    \n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_model = model_fit\n",
    "                        best_order = order\n",
    "                        best_result = {\n",
    "                            'station_code': station_code,\n",
    "                            'pollutant': pollutant,\n",
    "                            'order': order,\n",
    "                            'mae': mae,\n",
    "                            'mse': mse,\n",
    "                            'rmse': rmse,\n",
    "                            'neg_log_likelihood': neg_log_likelihood,\n",
    "                            'model': model_fit,\n",
    "                            'forecasts': forecasts if scaler is None else inverse_forecasts,\n",
    "                            'actual': test if scaler is None else original_test\n",
    "                        }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fitting ARIMA{order}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    grid_df = pd.DataFrame(grid_results)\n",
    "    \n",
    "    if not grid_df.empty:\n",
    "        try:\n",
    "            pivot_df = grid_df.pivot_table(\n",
    "                index='p', columns=['d', 'q'], values='RMSE'\n",
    "            )\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            sns.heatmap(pivot_df, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "            plt.title(f'RMSE for Different ARIMA Parameters - Station {station_code}, {pollutant}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"Station{station_code}_{pollutant}_parameter_grid.png\")\n",
    "            plt.show()\n",
    "        except:\n",
    "            print(\"Could not create heatmap visualization for parameter grid\")\n",
    "    \n",
    "    if best_model:\n",
    "        print(f\"\\nBest model for Station {station_code}, {pollutant}: ARIMA{best_order}\")\n",
    "        print(f\"MAE: {best_result['mae']:.4f}\")\n",
    "        print(f\"MSE: {best_result['mse']:.4f}\")\n",
    "        print(f\"RMSE: {best_result['rmse']:.4f}\")\n",
    "        print(f\"Negative Log-Likelihood: {best_result['neg_log_likelihood']:.4f}\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(best_result['actual'], 'b-o', markersize=3, label='Actual')\n",
    "        plt.plot(best_result['forecasts'], 'r--x', markersize=3, label='Forecast')\n",
    "        plt.title(f\"Best ARIMA{best_order} for Station {station_code}, {pollutant}\")\n",
    "        plt.xlabel('Test Sample Index')\n",
    "        plt.ylabel(f'{pollutant} Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"Station{station_code}_{pollutant}_best_arima.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        return best_result, grid_df\n",
    "    else:\n",
    "        print(f\"No successful models for Station {station_code}, {pollutant}\")\n",
    "        return None, None\n",
    "\n",
    "def compare_pollutants_across_stations(results_df, metric='RMSE'):\n",
    "    if results_df.empty:\n",
    "        print(\"No results to compare\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    pivot_df = results_df.pivot_table(\n",
    "        index='Station Code', columns='Pollutant', values=metric\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(pivot_df, annot=True, fmt='.2f', cmap='YlGnBu')\n",
    "    plt.title(f'{metric} Comparison Across Stations and Pollutants')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{metric}_comparison_heatmap.png\")\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        df = load_seoul_data()\n",
    "        \n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(df.info())\n",
    "        print(\"\\nMissing values per column:\")\n",
    "        print(df.isnull().sum())\n",
    "        \n",
    "        unique_stations = df['Station code'].unique()\n",
    "        available_pollutants = [col for col in df.columns \n",
    "                               if col in ['SO2', 'NO2', 'O3', 'CO', 'PM10', 'PM2.5']]\n",
    "        \n",
    "        print(f\"\\nFound {len(unique_stations)} stations: {unique_stations}\")\n",
    "        print(f\"Found {len(available_pollutants)} pollutants: {available_pollutants}\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        selected_station = unique_stations[0]\n",
    "        selected_pollutant = available_pollutants[0]\n",
    "        \n",
    "        # Normalized with Min-Max scaling\n",
    "        normalized_series, original_series, scaler, pollutant = prepare_time_series(\n",
    "            df, selected_station, selected_pollutant, normalize='minmax'\n",
    "        )\n",
    "        \n",
    "        analyze_time_series(normalized_series, selected_station, pollutant)\n",
    "        \n",
    "        result = run_arima_analysis(\n",
    "            normalized_series, original_series, scaler, \n",
    "            selected_station, pollutant\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            all_results.append({\n",
    "                'Station Code': selected_station,\n",
    "                'Pollutant': pollutant,\n",
    "                'ARIMA Order': str(result['order']),\n",
    "                'Normalization': 'MinMax',\n",
    "                'MAE': result['mae'],\n",
    "                'MSE': result['mse'],\n",
    "                'RMSE': result['rmse'],\n",
    "                'Neg_Log_Likelihood': result['neg_log_likelihood']\n",
    "            })\n",
    "        \n",
    "        # Normalized with Standard scaling (Z-score)\n",
    "        normalized_series, original_series, scaler, pollutant = prepare_time_series(\n",
    "            df, selected_station, selected_pollutant, normalize='standard'\n",
    "        )\n",
    "        \n",
    "        result = run_arima_analysis(\n",
    "            normalized_series, original_series, scaler, \n",
    "            selected_station, pollutant\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            all_results.append({\n",
    "                'Station Code': selected_station,\n",
    "                'Pollutant': pollutant,\n",
    "                'ARIMA Order': str(result['order']),\n",
    "                'Normalization': 'Standard',\n",
    "                'MAE': result['mae'],\n",
    "                'MSE': result['mse'],\n",
    "                'RMSE': result['rmse'],\n",
    "                'Neg_Log_Likelihood': result['neg_log_likelihood']\n",
    "            })\n",
    "        \n",
    "        # Without normalization\n",
    "        original_series, _, _, pollutant = prepare_time_series(\n",
    "            df, selected_station, selected_pollutant, normalize=None\n",
    "        )\n",
    "        \n",
    "        result = run_arima_analysis(\n",
    "            original_series, None, None, \n",
    "            selected_station, pollutant\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            all_results.append({\n",
    "                'Station Code': selected_station,\n",
    "                'Pollutant': pollutant,\n",
    "                'ARIMA Order': str(result['order']),\n",
    "                'Normalization': 'None',\n",
    "                'MAE': result['mae'],\n",
    "                'MSE': result['mse'],\n",
    "                'RMSE': result['rmse'],\n",
    "                'Neg_Log_Likelihood': result['neg_log_likelihood']\n",
    "            })\n",
    "        \n",
    "        best_result, grid_results = find_best_arima_model(\n",
    "            normalized_series, original_series, scaler,\n",
    "            selected_station, pollutant\n",
    "        )\n",
    "        \n",
    "        if best_result:\n",
    "            all_results.append({\n",
    "                'Station Code': selected_station,\n",
    "                'Pollutant': pollutant,\n",
    "                'ARIMA Order': f\"Best-{best_result['order']}\",\n",
    "                'Normalization': 'Standard',\n",
    "                'MAE': best_result['mae'],\n",
    "                'MSE': best_result['mse'],\n",
    "                'RMSE': best_result['rmse'],\n",
    "                'Neg_Log_Likelihood': best_result['neg_log_likelihood']\n",
    "            })\n",
    "        \n",
    "        subset_stations = unique_stations[:2]\n",
    "        subset_pollutants = available_pollutants[1:3]\n",
    "        \n",
    "        for station in subset_stations:\n",
    "            for pollutant in subset_pollutants:\n",
    "                if station == selected_station and pollutant == selected_pollutant:\n",
    "                    continue\n",
    "                \n",
    "                normalized_series, original_series, scaler, pollutant_name = prepare_time_series(\n",
    "                    df, station, pollutant, normalize='standard'\n",
    "                )\n",
    "                \n",
    "                result = run_arima_analysis(\n",
    "                    normalized_series, original_series, scaler,\n",
    "                    station, pollutant_name\n",
    "                )\n",
    "                \n",
    "                if result:\n",
    "                    all_results.append({\n",
    "                        'Station Code': station,\n",
    "                        'Pollutant': pollutant_name,\n",
    "                        'ARIMA Order': str(result['order']),\n",
    "                        'Normalization': 'Standard',\n",
    "                        'MAE': result['mae'],\n",
    "                        'MSE': result['mse'],\n",
    "                        'RMSE': result['rmse'],\n",
    "                        'Neg_Log_Likelihood': result['neg_log_likelihood']\n",
    "                    })\n",
    "        \n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        if not results_df.empty:\n",
    "            print(\"\\nResults Summary:\")\n",
    "            print(results_df)\n",
    "            \n",
    "            results_df.to_csv('seoul_arima_results.csv', index=False)\n",
    "            print(\"Results saved to 'seoul_arima_results.csv'\")\n",
    "            \n",
    "            if len(results_df) > 1:\n",
    "                compare_pollutants_across_stations(results_df, 'RMSE')\n",
    "                \n",
    "                plt.figure(figsize=(15, 8))\n",
    "                sns.barplot(x='Pollutant', y='RMSE', hue='Normalization', data=results_df)\n",
    "                plt.title('RMSE by Pollutant and Normalization Method')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('normalization_comparison.png')\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"No results to summarize\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beijing Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:15:47.358345Z",
     "iopub.status.busy": "2025-05-06T12:15:47.358024Z",
     "iopub.status.idle": "2025-05-06T12:24:36.060871Z",
     "shell.execute_reply": "2025-05-06T12:24:36.059867Z",
     "shell.execute_reply.started": "2025-05-06T12:15:47.358325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"muted\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "def load_beijing_data(data_path):\n",
    "    if os.path.isdir(data_path):\n",
    "        all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
    "        \n",
    "        if len(all_files) > 0:\n",
    "            print(f\"Found {len(all_files)} CSV files in directory\")\n",
    "            \n",
    "            df_list = []\n",
    "            \n",
    "            for file in all_files:\n",
    "                station_name = os.path.basename(file).replace('.csv', '')\n",
    "                \n",
    "                df = pd.read_csv(file)\n",
    "                \n",
    "                if 'station' not in df.columns:\n",
    "                    df['station'] = station_name\n",
    "                    \n",
    "                df_list.append(df)\n",
    "                print(f\"Loaded {station_name} with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "            \n",
    "            if len(df_list) > 0:\n",
    "                combined_df = pd.concat(df_list, ignore_index=True)\n",
    "            else:\n",
    "                raise ValueError(\"No valid CSV files found in directory\")\n",
    "        else:\n",
    "            raise ValueError(f\"No CSV files found in directory: {data_path}\")\n",
    "    \n",
    "    else:\n",
    "        if not os.path.exists(data_path):\n",
    "            possible_paths = [\n",
    "                '../input/beijing-multi-site-air-quality-data/PRSA_Data_20130301-20170228.csv',\n",
    "                '../input/beijing-multi-site-air-quality-data/Beijing_Multi_Site_Air_Quality.csv',\n",
    "                '../input/beijing-multi-site-air-quality-data/Beijing_AirQuality_Stations.csv',\n",
    "                './PRSA_Data_20130301-20170228.csv',\n",
    "                './Beijing_Multi_Site_Air_Quality.csv',\n",
    "                './Beijing_AirQuality_Stations.csv'\n",
    "            ]\n",
    "            \n",
    "            for p in possible_paths:\n",
    "                if os.path.exists(p):\n",
    "                    data_path = p\n",
    "                    print(f\"Found dataset at: {data_path}\")\n",
    "                    break\n",
    "            \n",
    "            if not os.path.exists(data_path):\n",
    "                raise ValueError(f\"Could not find dataset file. Please specify the correct path.\")\n",
    "        \n",
    "        print(f\"Loading single file: {data_path}\")\n",
    "        \n",
    "        try:\n",
    "            combined_df = pd.read_csv(data_path)\n",
    "            print(f\"Loaded combined file with {combined_df.shape[0]} rows and {combined_df.shape[1]} columns\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading dataset: {str(e)}\")\n",
    "    \n",
    "    required_columns = ['year', 'month', 'day', 'hour']\n",
    "    missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Dataset missing required columns: {missing_columns}\")\n",
    "    \n",
    "    if 'station' not in combined_df.columns:\n",
    "        if 'wd' in combined_df.columns and 'WSPM' in combined_df.columns:\n",
    "            print(\"No 'station' column found, but this appears to be a multi-station dataset.\")\n",
    "            if any(col.lower() == 'station' for col in combined_df.columns):\n",
    "                for col in combined_df.columns:\n",
    "                    if col.lower() == 'station':\n",
    "                        combined_df.rename(columns={col: 'station'}, inplace=True)\n",
    "                        break\n",
    "            else:\n",
    "                print(\"Warning: No station identifier found. Using 'Beijing' as default station.\")\n",
    "                combined_df['station'] = 'Beijing'\n",
    "    \n",
    "    if 'datetime' not in combined_df.columns:\n",
    "        combined_df['datetime'] = pd.to_datetime(combined_df[['year', 'month', 'day', 'hour']])\n",
    "    \n",
    "    print(f\"Final dataset has {combined_df.shape[0]} rows and {combined_df.shape[1]} columns\")\n",
    "    print(f\"Columns: {', '.join(combined_df.columns)}\")\n",
    "    \n",
    "    if 'station' in combined_df.columns:\n",
    "        stations = combined_df['station'].unique()\n",
    "        print(f\"Found {len(stations)} stations: {stations}\")\n",
    "    \n",
    "    pollutant_columns = [col for col in combined_df.columns \n",
    "                         if col in ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']]\n",
    "    print(f\"Found {len(pollutant_columns)} pollutant columns: {pollutant_columns}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def normalize_series(series, method='minmax'):\n",
    "    series_values = series.values.reshape(-1, 1)\n",
    "    if method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        return series, None\n",
    "    \n",
    "    normalized_values = scaler.fit_transform(series_values)\n",
    "    normalized_series = pd.Series(normalized_values.flatten(), index=series.index)\n",
    "    return normalized_series, scaler\n",
    "\n",
    "def prepare_time_series(df, station, pollutant, normalize_method=None):\n",
    "    try:\n",
    "        station_df = df[df['station'] == station].copy()\n",
    "        \n",
    "        if station_df.empty:\n",
    "            available_stations = df['station'].unique()\n",
    "            print(f\"No data found for station '{station}'. Available stations: {available_stations}\")\n",
    "            station = available_stations[0]\n",
    "            station_df = df[df['station'] == station].copy()\n",
    "            print(f\"Using station '{station}' instead\")\n",
    "        \n",
    "        if pollutant not in station_df.columns:\n",
    "            potential_pollutants = [col for col in station_df.columns \n",
    "                                   if col in ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']]\n",
    "            \n",
    "            if not potential_pollutants:\n",
    "                numeric_cols = station_df.select_dtypes(include=['number']).columns.tolist()\n",
    "                potential_pollutants = [col for col in numeric_cols \n",
    "                                       if col not in ['year', 'month', 'day', 'hour', 'No']]\n",
    "            \n",
    "            if potential_pollutants:\n",
    "                print(f\"Pollutant '{pollutant}' not found. Available pollutants: {potential_pollutants}\")\n",
    "                pollutant = potential_pollutants[0]\n",
    "                print(f\"Using pollutant '{pollutant}' instead\")\n",
    "            else:\n",
    "                raise ValueError(f\"No suitable pollutant columns found in the dataset\")\n",
    "        \n",
    "        station_df = station_df.sort_values('datetime')\n",
    "        station_df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        series = station_df[pollutant].copy()\n",
    "        series = series.dropna()\n",
    "        \n",
    "        original_series = series.copy()\n",
    "        scaler = None\n",
    "        \n",
    "        if normalize_method:\n",
    "            series, scaler = normalize_series(series, normalize_method)\n",
    "            print(f\"Applied {normalize_method} normalization to {pollutant}\")\n",
    "            print(f\"Original range: {original_series.min():.4f} to {original_series.max():.4f}\")\n",
    "            print(f\"Normalized range: {series.min():.4f} to {series.max():.4f}\")\n",
    "        \n",
    "        print(f\"Prepared time series for {pollutant} at {station}\")\n",
    "        print(f\"Time series length: {len(series)}\")\n",
    "        if not series.empty:\n",
    "            print(f\"Time range: {series.index.min()} to {series.index.max()}\")\n",
    "        \n",
    "        return series, original_series, scaler, pollutant\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing time series: {str(e)}\")\n",
    "        return pd.Series([]), pd.Series([]), None, pollutant\n",
    "\n",
    "def analyze_time_series(series, pollutant, station):\n",
    "    if len(series) < 10:\n",
    "        print(\"Not enough data points for time series analysis\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(series)\n",
    "    plt.title(f'Time Series - {pollutant} at {station}')\n",
    "    plt.ylabel(f'{pollutant} Concentration')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    ax2 = plt.subplot(3, 1, 2)\n",
    "    plot_acf(series, ax=ax2, lags=40)\n",
    "    plt.title('Autocorrelation Function')\n",
    "    \n",
    "    ax3 = plt.subplot(3, 1, 3)\n",
    "    plot_pacf(series, ax=ax3, lags=40)\n",
    "    plt.title('Partial Autocorrelation Function')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{pollutant}_{station}_time_series_analysis.png\".replace(' ', '_'))\n",
    "    plt.show()\n",
    "\n",
    "def run_arima_analysis(series, original_series, scaler, pollutant, station, order=(1,1,1), test_size=0.2):\n",
    "    if len(series) < 30:\n",
    "        print(f\"Not enough data points for {pollutant} at {station}. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    train_size = int(len(series) * (1 - test_size))\n",
    "    train, test = series.iloc[:train_size].values, series.iloc[train_size:].values\n",
    "    original_test = original_series.iloc[train_size:].values if original_series is not None else None\n",
    "    \n",
    "    print(f\"\\nAnalyzing {pollutant} at {station} with ARIMA{order}\")\n",
    "    print(f\"Training data size: {len(train)}\")\n",
    "    print(f\"Test data size: {len(test)}\")\n",
    "    \n",
    "    try:\n",
    "        model = ARIMA(train, order=order)\n",
    "        model_fit = model.fit()\n",
    "        \n",
    "        forecasts = model_fit.forecast(steps=len(test))\n",
    "        \n",
    "        if scaler is not None:\n",
    "            forecasts_2d = forecasts.reshape(-1, 1)\n",
    "            inverse_forecasts = scaler.inverse_transform(forecasts_2d)\n",
    "            inverse_forecasts = inverse_forecasts.flatten()\n",
    "            \n",
    "            mae = mean_absolute_error(original_test, inverse_forecasts)\n",
    "            mse = mean_squared_error(original_test, inverse_forecasts)\n",
    "            rmse = np.sqrt(mse)\n",
    "            forecasts_for_plot = inverse_forecasts\n",
    "            actual_for_plot = original_test\n",
    "            print(\"Metrics calculated on original scale data\")\n",
    "        else:\n",
    "            mae = mean_absolute_error(test, forecasts)\n",
    "            mse = mean_squared_error(test, forecasts)\n",
    "            rmse = np.sqrt(mse)\n",
    "            forecasts_for_plot = forecasts\n",
    "            actual_for_plot = test\n",
    "        \n",
    "        neg_log_likelihood = -model_fit.llf\n",
    "        \n",
    "        print(f\"Results for {pollutant} at {station} - ARIMA{order}:\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"Negative Log-Likelihood: {neg_log_likelihood:.4f}\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(actual_for_plot, label='Actual', color='blue', marker='o', markersize=3, linestyle='-', linewidth=1)\n",
    "        plt.plot(forecasts_for_plot, label='Forecast', color='red', marker='x', markersize=3, linestyle='--', linewidth=1)\n",
    "        norm_text = f\" ({scaler.__class__.__name__})\" if scaler else \"\"\n",
    "        plt.title(f'ARIMA{order} Forecast vs Actual for {pollutant} at {station}{norm_text}')\n",
    "        plt.xlabel('Test Sample Index')\n",
    "        plt.ylabel(f'{pollutant} Concentration')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{pollutant}_{station}_arima_forecast{norm_text}.png\".replace(' ', '_'))\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'pollutant': pollutant,\n",
    "            'station': station,\n",
    "            'order': order,\n",
    "            'normalize_method': scaler.__class__.__name__ if scaler else 'None',\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'neg_log_likelihood': neg_log_likelihood,\n",
    "            'model': model_fit,\n",
    "            'forecasts': forecasts_for_plot,\n",
    "            'actual': actual_for_plot\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting ARIMA for {pollutant} at {station}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def find_best_arima_model(series, original_series, scaler, pollutant, station, p_range=(0,2), d_range=(0,2), q_range=(0,2)):\n",
    "    if len(series) < 30:\n",
    "        print(f\"Not enough data points for {pollutant} at {station}. Skipping.\")\n",
    "        return None, None\n",
    "    \n",
    "    train_size = int(len(series) * 0.8)\n",
    "    train, test = series.iloc[:train_size].values, series.iloc[train_size:].values\n",
    "    original_test = original_series.iloc[train_size:].values if original_series is not None else None\n",
    "    \n",
    "    print(f\"\\nFinding best ARIMA model for {pollutant} at {station}\")\n",
    "    print(f\"Training data size: {len(train)}\")\n",
    "    print(f\"Test data size: {len(test)}\")\n",
    "    \n",
    "    grid_results = []\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    best_order = None\n",
    "    best_result = None\n",
    "    \n",
    "    for p in range(p_range[0], p_range[1] + 1):\n",
    "        for d in range(d_range[0], d_range[1] + 1):\n",
    "            for q in range(q_range[0], q_range[1] + 1):\n",
    "                try:\n",
    "                    order = (p, d, q)\n",
    "                    model = ARIMA(train, order=order)\n",
    "                    model_fit = model.fit()\n",
    "                    \n",
    "                    forecasts = model_fit.forecast(steps=len(test))\n",
    "                    \n",
    "                    if scaler is not None:\n",
    "                        forecasts_2d = forecasts.reshape(-1, 1)\n",
    "                        inverse_forecasts = scaler.inverse_transform(forecasts_2d)\n",
    "                        inverse_forecasts = inverse_forecasts.flatten()\n",
    "                        \n",
    "                        mae = mean_absolute_error(original_test, inverse_forecasts)\n",
    "                        mse = mean_squared_error(original_test, inverse_forecasts)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                        forecasts_for_save = inverse_forecasts\n",
    "                        actual_for_save = original_test\n",
    "                    else:\n",
    "                        mae = mean_absolute_error(test, forecasts)\n",
    "                        mse = mean_squared_error(test, forecasts)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                        forecasts_for_save = forecasts\n",
    "                        actual_for_save = test\n",
    "                    \n",
    "                    neg_log_likelihood = -model_fit.llf\n",
    "                    \n",
    "                    print(f\"ARIMA{order} - RMSE: {rmse:.4f}, NegLogLike: {neg_log_likelihood:.4f}\")\n",
    "                    \n",
    "                    grid_results.append({\n",
    "                        'p': p,\n",
    "                        'd': d,\n",
    "                        'q': q,\n",
    "                        'RMSE': rmse,\n",
    "                        'MAE': mae,\n",
    "                        'MSE': mse,\n",
    "                        'Neg_Log_Likelihood': neg_log_likelihood\n",
    "                    })\n",
    "                    \n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_model = model_fit\n",
    "                        best_order = order\n",
    "                        best_result = {\n",
    "                            'pollutant': pollutant,\n",
    "                            'station': station,\n",
    "                            'order': order,\n",
    "                            'normalize_method': scaler.__class__.__name__ if scaler else 'None',\n",
    "                            'mae': mae,\n",
    "                            'mse': mse,\n",
    "                            'rmse': rmse,\n",
    "                            'neg_log_likelihood': neg_log_likelihood,\n",
    "                            'model': model_fit,\n",
    "                            'forecasts': forecasts_for_save,\n",
    "                            'actual': actual_for_save\n",
    "                        }\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error fitting ARIMA{order}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    grid_df = pd.DataFrame(grid_results)\n",
    "    \n",
    "    if not grid_df.empty:\n",
    "        if len(grid_df) > 1:\n",
    "            try:\n",
    "                pivot_df = grid_df.pivot_table(\n",
    "                    index='p', columns=['d', 'q'], values='RMSE'\n",
    "                )\n",
    "                plt.figure(figsize=(15, 10))\n",
    "                sns.heatmap(pivot_df, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "                norm_text = f\" ({scaler.__class__.__name__})\" if scaler else \"\"\n",
    "                plt.title(f'RMSE for Different ARIMA Parameters - {pollutant} at {station}{norm_text}')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{pollutant}_{station}_parameter_grid{norm_text}.png\".replace(' ', '_'))\n",
    "                plt.show()\n",
    "            except:\n",
    "                print(\"Could not create heatmap visualization\")\n",
    "    \n",
    "    if best_model:\n",
    "        print(f\"\\nBest model for {pollutant} at {station}: ARIMA{best_order}\")\n",
    "        print(f\"MAE: {best_result['mae']:.4f}\")\n",
    "        print(f\"MSE: {best_result['mse']:.4f}\")\n",
    "        print(f\"RMSE: {best_result['rmse']:.4f}\")\n",
    "        print(f\"Negative Log-Likelihood: {best_result['neg_log_likelihood']:.4f}\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(best_result['actual'], 'b-o', markersize=3, label='Actual')\n",
    "        plt.plot(best_result['forecasts'], 'r--x', markersize=3, label='Forecast')\n",
    "        norm_text = f\" ({scaler.__class__.__name__})\" if scaler else \"\"\n",
    "        plt.title(f\"Best ARIMA{best_order} for {pollutant} at {station}{norm_text}\")\n",
    "        plt.xlabel('Test Sample Index')\n",
    "        plt.ylabel(f'{pollutant} Concentration')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{pollutant}_{station}_best_arima{norm_text}.png\".replace(' ', '_'))\n",
    "        plt.show()\n",
    "        \n",
    "        return best_result, grid_df\n",
    "    else:\n",
    "        print(f\"No successful models for {pollutant} at {station}\")\n",
    "        return None, None\n",
    "\n",
    "def compare_results(results_df, metric='RMSE'):\n",
    "    if results_df.empty:\n",
    "        print(\"No results to compare\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    if 'Station' in results_df.columns and 'Normalization' in results_df.columns:\n",
    "        pivot_df = results_df.pivot_table(\n",
    "            index='Station', columns=['Pollutant', 'Normalization'], values=metric\n",
    "        )\n",
    "    elif 'Station' in results_df.columns:\n",
    "        pivot_df = results_df.pivot_table(\n",
    "            index='Station', columns='Pollutant', values=metric\n",
    "        )\n",
    "    else:\n",
    "        pivot_df = results_df.pivot_table(\n",
    "            index='Pollutant', columns='Normalization', values=metric\n",
    "        )\n",
    "    \n",
    "    sns.heatmap(pivot_df, annot=True, fmt='.2f', cmap='YlGnBu')\n",
    "    plt.title(f'{metric} Comparison')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{metric}_comparison_heatmap.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    if 'Normalization' in results_df.columns:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        sns.barplot(x='Pollutant', y=metric, hue='Normalization', data=results_df)\n",
    "        plt.title(f'{metric} by Pollutant and Normalization Method')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{metric}_normalization_comparison.png\")\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        df = load_beijing_data('/kaggle/input/beijing-multisite-airquality-data-set')\n",
    "        \n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(df.info())\n",
    "        print(\"\\nMissing values per column:\")\n",
    "        print(df.isnull().sum())\n",
    "        \n",
    "        pollutants = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']\n",
    "        stations = df['station'].unique()\n",
    "        print(f\"\\nFound {len(stations)} stations: {stations}\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        selected_pollutant = 'PM2.5'\n",
    "        selected_station = stations[0]\n",
    "        \n",
    "        for normalize_method in [None, 'minmax', 'standard']:\n",
    "            norm_name = normalize_method if normalize_method else 'none'\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Analyzing with {norm_name} normalization\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            series, original_series, scaler, pollutant = prepare_time_series(\n",
    "                df, selected_station, selected_pollutant, normalize_method\n",
    "            )\n",
    "            \n",
    "            if normalize_method is None:\n",
    "                analyze_time_series(series, pollutant, selected_station)\n",
    "            \n",
    "            result = run_arima_analysis(\n",
    "                series, original_series, scaler, \n",
    "                pollutant, selected_station\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                all_results.append({\n",
    "                    'Pollutant': pollutant,\n",
    "                    'Station': selected_station,\n",
    "                    'ARIMA_Order': str(result['order']),\n",
    "                    'Normalization': result['normalize_method'],\n",
    "                    'MAE': result['mae'],\n",
    "                    'MSE': result['mse'],\n",
    "                    'RMSE': result['rmse'],\n",
    "                    'Neg_Log_Likelihood': result['neg_log_likelihood']\n",
    "                })\n",
    "            \n",
    "            best_result, grid_results = find_best_arima_model(\n",
    "                series, original_series, scaler,\n",
    "                pollutant, selected_station\n",
    "            )\n",
    "            \n",
    "            if best_result:\n",
    "                all_results.append({\n",
    "                    'Pollutant': pollutant,\n",
    "                    'Station': selected_station,\n",
    "                    'ARIMA_Order': f\"Best-{best_result['order']}\",\n",
    "                    'Normalization': best_result['normalize_method'],\n",
    "                    'MAE': best_result['mae'],\n",
    "                    'MSE': best_result['mse'],\n",
    "                    'RMSE': best_result['rmse'],\n",
    "                    'Neg_Log_Likelihood': best_result['neg_log_likelihood']\n",
    "                })\n",
    "        \n",
    "        subset_stations = stations[:2]\n",
    "        subset_pollutants = pollutants[1:3]\n",
    "        \n",
    "        for station in subset_stations:\n",
    "            if station == selected_station:\n",
    "                continue\n",
    "            \n",
    "            for pollutant in subset_pollutants:\n",
    "                series, original_series, scaler, pollutant_name = prepare_time_series(\n",
    "                    df, station, pollutant, normalize_method='standard'\n",
    "                )\n",
    "                \n",
    "                result = run_arima_analysis(\n",
    "                    series, original_series, scaler,\n",
    "                    pollutant_name, station\n",
    "                )\n",
    "                \n",
    "                if result:\n",
    "                    all_results.append({\n",
    "                        'Pollutant': pollutant_name,\n",
    "                        'Station': station,\n",
    "                        'ARIMA_Order': str(result['order']),\n",
    "                        'Normalization': result['normalize_method'],\n",
    "                        'MAE': result['mae'],\n",
    "                        'MSE': result['mse'],\n",
    "                        'RMSE': result['rmse'],\n",
    "                        'Neg_Log_Likelihood': result['neg_log_likelihood']\n",
    "                    })\n",
    "        \n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        if not results_df.empty:\n",
    "            print(\"\\nResults Summary:\")\n",
    "            print(results_df)\n",
    "            \n",
    "            results_df.to_csv('beijing_arima_results.csv', index=False)\n",
    "            print(\"Results saved to 'beijing_arima_results.csv'\")\n",
    "            \n",
    "            compare_results(results_df, 'RMSE')\n",
    "            compare_results(results_df, 'MAE')\n",
    "        else:\n",
    "            print(\"No results to summarize\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 409180,
     "sourceId": 783762,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7343892,
     "sourceId": 11700172,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 576393,
     "sourceId": 1056900,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
