{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk750CNSfJIe",
        "outputId": "6c7d008d-fcef-41fc-afb0-ef3b82e836ac"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eulvMtGRv4u8"
      },
      "source": [
        "#### Dublin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "r7-xe-Bgv7Wy",
        "outputId": "79b5f748-e58c-4cb4-8145-052e684d6d02"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch numpy pandas scikit-learn matplotlib tqdm joblib\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "import glob\n",
        "import time\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "data_path = \"data/airview_dublincity_roaddata_ugm3.csv\"\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
        "    \n",
        "    print(f\"Data file not found at {data_path}.\")\n",
        "    print(\"Please upload the data file or provide the correct path.\")\n",
        "    print(\"For now, creating a mock dataset for demonstration purposes...\")\n",
        "    \n",
        "    from datetime import datetime, timedelta\n",
        "    \n",
        "    start_date = datetime(2021, 1, 1)\n",
        "    n_samples = 8760\n",
        "    dates = [start_date + timedelta(hours=i) for i in range(n_samples)]\n",
        "    \n",
        "    pollutants = ['NO2', 'PM2.5', 'PM10', 'BC', 'UFP']\n",
        "    mock_data = {\n",
        "        'timestamp': dates\n",
        "    }\n",
        "    \n",
        "    for pollutant in pollutants:\n",
        "        base = np.random.normal(30, 10, n_samples)\n",
        "        seasonal = 15 * np.sin(np.linspace(0, 4*np.pi, n_samples))\n",
        "        trend = np.linspace(0, 10, n_samples)\n",
        "        noise = np.random.normal(0, 5, n_samples)\n",
        "        values = base + seasonal + trend + noise\n",
        "        values = np.maximum(0, values)\n",
        "        mock_data[pollutant] = values\n",
        "    \n",
        "    df = pd.DataFrame(mock_data)\n",
        "    df.to_csv(data_path, index=False)\n",
        "    print(f\"Mock dataset created and saved to {data_path}\")\n",
        "\n",
        "print(f\"Loading data from {data_path}\")\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "print(\"\\nDataset Information:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nSummary statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(missing_values)\n",
        "\n",
        "timestamp_col = None\n",
        "for col in df.columns:\n",
        "    if 'time' in col.lower() or 'date' in col.lower():\n",
        "        timestamp_col = col\n",
        "        break\n",
        "\n",
        "if timestamp_col is None:\n",
        "    timestamp_col = df.columns[0]\n",
        "    print(f\"\\nAssuming '{timestamp_col}' is the timestamp column\")\n",
        "else:\n",
        "    print(f\"\\nIdentified '{timestamp_col}' as the timestamp column\")\n",
        "\n",
        "df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
        "df = df.sort_values(by=timestamp_col)\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "if timestamp_col in numeric_cols:\n",
        "    numeric_cols.remove(timestamp_col)\n",
        "\n",
        "print(f\"\\nIdentified {len(numeric_cols)} numeric columns as potential pollutants/features:\")\n",
        "print(numeric_cols)\n",
        "\n",
        "if df.isnull().sum().sum() > 0:\n",
        "    print(\"\\nFilling missing values...\")\n",
        "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
        "    print(f\"Missing values after filling: {df.isnull().sum().sum()}\")\n",
        "\n",
        "print(\"\\nCreating enhanced time-based features...\")\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df[timestamp_col].dt.hour / 24)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df[timestamp_col].dt.hour / 24)\n",
        "df['day_sin'] = np.sin(2 * np.pi * df[timestamp_col].dt.day / 31)\n",
        "df['day_cos'] = np.cos(2 * np.pi * df[timestamp_col].dt.day / 31)\n",
        "df['month_sin'] = np.sin(2 * np.pi * df[timestamp_col].dt.month / 12)\n",
        "df['month_cos'] = np.cos(2 * np.pi * df[timestamp_col].dt.month / 12)\n",
        "df['dayofweek_sin'] = np.sin(2 * np.pi * df[timestamp_col].dt.dayofweek / 7)\n",
        "df['dayofweek_cos'] = np.cos(2 * np.pi * df[timestamp_col].dt.dayofweek / 7)\n",
        "\n",
        "print(\"Adding lag features...\")\n",
        "for col in numeric_cols[:5]:\n",
        "    for lag in [1, 3, 6, 12, 24]:\n",
        "        lag_col = f\"{col}_lag_{lag}\"\n",
        "        df[lag_col] = df[col].shift(lag)\n",
        "\n",
        "df = df.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "seq_len = 24\n",
        "pred_len = 12\n",
        "target_cols = numeric_cols[:5]\n",
        "print(f\"\\nUsing {target_cols} as target columns\")\n",
        "\n",
        "cols_to_scale = [col for col in df.columns if col != timestamp_col and pd.api.types.is_numeric_dtype(df[col])]\n",
        "scaler = StandardScaler()\n",
        "df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
        "\n",
        "joblib.dump(scaler, \"dublin_air_scaler.pkl\")\n",
        "print(\"Scaler saved to dublin_air_scaler.pkl\")\n",
        "\n",
        "class AirQualityDataset(Dataset):\n",
        "    def __init__(self, data, seq_len, pred_len, target_indices):\n",
        "        self.data = data.values\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.target_indices = target_indices\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len - self.pred_len + 1\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        x_start = idx\n",
        "        x_end = idx + self.seq_len\n",
        "        x = self.data[x_start:x_end]\n",
        "        \n",
        "        y_start = x_end\n",
        "        y_end = y_start + self.pred_len\n",
        "        y = self.data[y_start:y_end][:, self.target_indices]\n",
        "        \n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "data_array = df[cols_to_scale]\n",
        "target_indices = [cols_to_scale.index(col) for col in target_cols]\n",
        "\n",
        "train_data, temp_data = train_test_split(data_array, test_size=0.3, shuffle=False)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
        "\n",
        "print(f\"\\nTraining data size: {len(train_data)}\")\n",
        "print(f\"Validation data size: {len(val_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")\n",
        "\n",
        "print(f\"\\nCreating datasets with seq_len={seq_len}, pred_len={pred_len}\")\n",
        "print(f\"Total data length: {len(data_array)}\")\n",
        "\n",
        "expected_train_samples = len(train_data) - seq_len - pred_len + 1\n",
        "expected_val_samples = len(val_data) - seq_len - pred_len + 1\n",
        "expected_test_samples = len(test_data) - seq_len - pred_len + 1\n",
        "\n",
        "print(f\"Expected training samples: {expected_train_samples}\")\n",
        "print(f\"Expected validation samples: {expected_val_samples}\")\n",
        "print(f\"Expected test samples: {expected_test_samples}\")\n",
        "\n",
        "train_dataset = AirQualityDataset(train_data, seq_len, pred_len, target_indices)\n",
        "val_dataset = AirQualityDataset(val_data, seq_len, pred_len, target_indices)\n",
        "test_dataset = AirQualityDataset(test_data, seq_len, pred_len, target_indices)\n",
        "\n",
        "print(f\"Actual training samples: {len(train_dataset)}\")\n",
        "print(f\"Actual validation samples: {len(val_dataset)}\")\n",
        "print(f\"Actual test samples: {len(test_dataset)}\")\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"\\nNumber of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader)}\")\n",
        "print(f\"Number of test batches: {len(test_loader)}\")\n",
        "\n",
        "print(\"\\nChecking data loader...\")\n",
        "sample_x, sample_y = next(iter(train_loader))\n",
        "print(f\"Sample input shape: {sample_x.shape}, Sample target shape: {sample_y.shape}\")\n",
        "\n",
        "class TimeSeriesForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, seq_len, pred_len):\n",
        "        super(TimeSeriesForecaster, self).__init__()\n",
        "        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=3,\n",
        "            batch_first=True,\n",
        "            dropout=0.2,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        \n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_dim // 2)\n",
        "        self.fc3 = nn.Linear(hidden_dim // 2, pred_len * output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.output_dim = output_dim\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        x = self.input_embedding(x)\n",
        "        \n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        \n",
        "        attn_weights = self.attention(lstm_out).squeeze(-1)\n",
        "        attn_weights = torch.softmax(attn_weights, dim=1).unsqueeze(2)\n",
        "        \n",
        "        context = torch.sum(lstm_out * attn_weights, dim=1)\n",
        "        \n",
        "        x = self.fc1(context)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.fc2(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        output = x.reshape(batch_size, self.pred_len, self.output_dim)\n",
        "        return output\n",
        "\n",
        "input_dim = sample_x.shape[2]\n",
        "hidden_dim = 512\n",
        "output_dim = len(target_cols)\n",
        "\n",
        "model = TimeSeriesForecaster(input_dim, hidden_dim, output_dim, seq_len, pred_len).to(device)\n",
        "print(f\"\\nModel created with input_dim={input_dim}, hidden_dim={hidden_dim}, output_dim={output_dim}\")\n",
        "\n",
        "class WeightedMSELoss(nn.Module):\n",
        "    def __init__(self, alpha=1.5):\n",
        "        super(WeightedMSELoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "        \n",
        "    def forward(self, pred, target):\n",
        "        mse_loss = self.mse(pred, target)\n",
        "        \n",
        "        batch_size, seq_len, feat_dim = pred.shape\n",
        "        time_weights = torch.linspace(1.0, self.alpha, seq_len).view(1, -1, 1).to(pred.device)\n",
        "        time_weights = time_weights.expand(batch_size, seq_len, feat_dim)\n",
        "        \n",
        "        weighted_loss = (mse_loss * time_weights).mean()\n",
        "        return weighted_loss\n",
        "\n",
        "criterion = WeightedMSELoss(alpha=2.0)\n",
        "\n",
        "print(\"\\nInitializing optimizer and verifying parameters...\")\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "param_count = 0\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        param_count += 1\n",
        "        if param_count <= 5:\n",
        "            print(f\"  {name}: {param.shape}\")\n",
        "\n",
        "if param_count == 0:\n",
        "    print(\"WARNING: No trainable parameters found!\")\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, verbose=True, min_lr=1e-6)\n",
        "\n",
        "if len(optimizer.param_groups[0]['params']) == 0:\n",
        "    print(\"WARNING: Optimizer has no parameters!\")\n",
        "else:\n",
        "    print(f\"Optimizer initialized with {len(optimizer.param_groups[0]['params'])} parameter groups\")\n",
        "\n",
        "epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "patience = 8\n",
        "counter = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"\\nStarting training with detailed monitoring...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "    batch_count = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_iter):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        if batch_idx == 0:\n",
        "            print(f\"Batch data shape: {data.shape}, target shape: {target.shape}\")\n",
        "        \n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        output = model(data)\n",
        "        \n",
        "        loss = criterion(output, target)\n",
        "        current_loss = loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += current_loss\n",
        "        batch_count += 1\n",
        "        \n",
        "        train_iter.set_postfix({\n",
        "            \"loss\": current_loss,\n",
        "            \"avg_loss\": train_loss / (batch_idx + 1),\n",
        "            \"batch\": f\"{batch_idx+1}/{len(train_loader)}\"\n",
        "        })\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx}/{len(train_loader)}: Loss = {current_loss:.6f}\")\n",
        "    \n",
        "    train_loss /= batch_count\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_batch_count = 0\n",
        "    val_iter = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Valid]\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_iter):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            current_loss = loss.item()\n",
        "            \n",
        "            val_loss += current_loss\n",
        "            val_batch_count += 1\n",
        "            \n",
        "            val_iter.set_postfix({\n",
        "                \"loss\": current_loss,\n",
        "                \"avg_loss\": val_loss / (batch_idx + 1),\n",
        "                \"batch\": f\"{batch_idx+1}/{len(val_loader)}\"\n",
        "            })\n",
        "    \n",
        "    val_loss /= val_batch_count\n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    scheduler.step(val_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.6f} ({batch_count} batches)\")\n",
        "    print(f\"  Val Loss: {val_loss:.6f} ({val_batch_count} batches)\")\n",
        "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_dublin_model.pt\")\n",
        "        print(f\"  Saved model with validation loss: {val_loss:.6f}\")\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"  No improvement for {counter} epochs (best val loss: {best_val_loss:.6f})\")\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('dublin_loss_curves.png')\n",
        "plt.show()\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_dublin_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "print(\"\\nEvaluating model on test data...\")\n",
        "test_loss = 0\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "all_inputs = []\n",
        "\n",
        "batch_times = []\n",
        "inference_times = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
        "        start_time = time.time()\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        forward_start = time.time()\n",
        "        output = model(data)\n",
        "        inference_time = time.time() - forward_start\n",
        "        inference_times.append(inference_time)\n",
        "        \n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item()\n",
        "        \n",
        "        all_preds.append(output.cpu().numpy())\n",
        "        all_targets.append(target.cpu().numpy())\n",
        "        all_inputs.append(data.cpu().numpy())\n",
        "        \n",
        "        batch_time = time.time() - start_time\n",
        "        batch_times.append(batch_time)\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx}: Loss={loss.item():.6f}, Time={batch_time:.4f}s, Inference={inference_time:.4f}s\")\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "print(f\"Test Loss: {test_loss:.6f}\")\n",
        "print(f\"Average batch processing time: {np.mean(batch_times):.4f}s\")\n",
        "print(f\"Average inference time: {np.mean(inference_times):.4f}s\")\n",
        "\n",
        "all_preds = np.concatenate(all_preds, axis=0)\n",
        "all_targets = np.concatenate(all_targets, axis=0)\n",
        "all_inputs = np.concatenate(all_inputs, axis=0)\n",
        "\n",
        "original_target_indices = [cols_to_scale.index(col) for col in target_cols]\n",
        "\n",
        "unscaled_preds = np.zeros_like(all_preds)\n",
        "unscaled_targets = np.zeros_like(all_targets)\n",
        "\n",
        "for i, idx in enumerate(original_target_indices):\n",
        "    for h in range(pred_len):\n",
        "        temp_pred = np.zeros((all_preds.shape[0], len(cols_to_scale)))\n",
        "        temp_pred[:, idx] = all_preds[:, h, i]\n",
        "        \n",
        "        unscaled_temp = scaler.inverse_transform(temp_pred)\n",
        "        unscaled_preds[:, h, i] = unscaled_temp[:, idx]\n",
        "        \n",
        "        temp_target = np.zeros((all_targets.shape[0], len(cols_to_scale)))\n",
        "        temp_target[:, idx] = all_targets[:, h, i]\n",
        "        \n",
        "        unscaled_temp = scaler.inverse_transform(temp_target)\n",
        "        unscaled_targets[:, h, i] = unscaled_temp[:, idx]\n",
        "\n",
        "print(\"\\nCalculating evaluation metrics...\")\n",
        "metrics_per_hour = []\n",
        "for hour in range(pred_len):\n",
        "    hour_metrics = []\n",
        "    for i, pollutant in enumerate(target_cols):\n",
        "        mae = mean_absolute_error(unscaled_targets[:, hour, i], unscaled_preds[:, hour, i])\n",
        "        rmse = np.sqrt(mean_squared_error(unscaled_targets[:, hour, i], unscaled_preds[:, hour, i]))\n",
        "        r2 = r2_score(unscaled_targets[:, hour, i], unscaled_preds[:, hour, i])\n",
        "        hour_metrics.append({\n",
        "            'Hour': hour + 1,\n",
        "            'Pollutant': pollutant,\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'R2': r2\n",
        "        })\n",
        "    metrics_per_hour.extend(hour_metrics)\n",
        "\n",
        "hourly_metrics_df = pd.DataFrame(metrics_per_hour)\n",
        "print(\"\\nHourly Prediction Metrics (first 12 rows):\")\n",
        "print(hourly_metrics_df.head(12))\n",
        "\n",
        "pivot_df = hourly_metrics_df.pivot_table(index='Hour', columns='Pollutant', values='RMSE')\n",
        "plt.figure(figsize=(14, 8))\n",
        "for pollutant in pivot_df.columns:\n",
        "    plt.plot(pivot_df.index, pivot_df[pollutant], marker='o', label=pollutant)\n",
        "plt.xlabel('Prediction Hour')\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('RMSE by Hour for Each Pollutant')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(range(1, pred_len + 1))\n",
        "plt.savefig('dublin_rmse_by_hour.png')\n",
        "plt.show()\n",
        "\n",
        "num_samples = 3\n",
        "fig, axes = plt.subplots(len(target_cols), num_samples, figsize=(18, 12), sharex=True)\n",
        "\n",
        "for i, pollutant in enumerate(target_cols):\n",
        "    sample_indices = np.random.choice(len(unscaled_preds), num_samples, replace=False)\n",
        "    \n",
        "    for j, idx in enumerate(sample_indices):\n",
        "        ax = axes[i, j]\n",
        "        ax.plot(range(pred_len), unscaled_preds[idx, :, i], 'r-', linewidth=2, label='Predicted')\n",
        "        ax.plot(range(pred_len), unscaled_targets[idx, :, i], 'b-', linewidth=2, label='Actual')\n",
        "        ax.set_title(f'{pollutant} - Sample {j+1}')\n",
        "        ax.grid(True)\n",
        "        \n",
        "        if i == len(target_cols) - 1:\n",
        "            ax.set_xlabel('Hours Ahead')\n",
        "        \n",
        "        if j == 0:\n",
        "            ax.set_ylabel(f'{pollutant} Level')\n",
        "            \n",
        "        if i == 0 and j == 0:\n",
        "            ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('dublin_prediction_samples.png')\n",
        "plt.show()\n",
        "\n",
        "overall_metrics = []\n",
        "for i, pollutant in enumerate(target_cols):\n",
        "    mae = mean_absolute_error(unscaled_targets[:, :, i].flatten(), unscaled_preds[:, :, i].flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(unscaled_targets[:, :, i].flatten(), unscaled_preds[:, :, i].flatten()))\n",
        "    r2 = r2_score(unscaled_targets[:, :, i].flatten(), unscaled_preds[:, :, i].flatten())\n",
        "    \n",
        "    overall_metrics.append({\n",
        "        'Pollutant': pollutant,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'R2': r2\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(overall_metrics)\n",
        "print(\"\\nOverall Metrics by Pollutant:\")\n",
        "print(metrics_df)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "metrics_df.plot(x='Pollutant', y=['MAE', 'RMSE'], kind='bar', ax=plt.gca())\n",
        "plt.title('MAE and RMSE by Pollutant')\n",
        "plt.ylabel('Error Value')\n",
        "plt.grid(axis='y')\n",
        "plt.savefig('dublin_error_by_pollutant.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics_df['Pollutant'], metrics_df['R2'])\n",
        "plt.title('R² Score by Pollutant')\n",
        "plt.ylabel('R² Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y')\n",
        "plt.savefig('dublin_r2_by_pollutant.png')\n",
        "plt.show()\n",
        "\n",
        "sample_input = next(iter(test_loader))[0][0].unsqueeze(0)\n",
        "torch.save(sample_input, \"dublin_sample_input.pt\")\n",
        "\n",
        "print(\"\\nSummary of Results:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(\"\\nAverage Metrics Across All Pollutants:\")\n",
        "print(f\"Average MAE: {metrics_df['MAE'].mean():.4f}\")\n",
        "print(f\"Average RMSE: {metrics_df['RMSE'].mean():.4f}\")\n",
        "print(f\"Average R²: {metrics_df['R2'].mean():.4f}\")\n",
        "\n",
        "print(\"\\nTraining and evaluation completed successfully!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
