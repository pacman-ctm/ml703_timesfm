{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk750CNSfJIe",
        "outputId": "6c7d008d-fcef-41fc-afb0-ef3b82e836ac"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Seoul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0nHe0RuoA7b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import os\n",
        "import glob\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/data/AirPollutionSeoul\"\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 50\n",
        "SEQUENCE_LENGTH = 24\n",
        "HIDDEN_SIZE = 64\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "\n",
        "class AirQualityDataset(Dataset):\n",
        "    def __init__(self, features, targets, seq_length=SEQUENCE_LENGTH):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx:idx + self.seq_length]\n",
        "        y = self.targets[idx + self.seq_length]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "def load_and_preprocess_seoul_data(data_dir):\n",
        "    try:\n",
        "        print(f\"Looking for CSV files in {data_dir}\")\n",
        "        csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n",
        "\n",
        "        if not csv_files:\n",
        "            raise FileNotFoundError(f\"No CSV files found in {data_dir}\")\n",
        "\n",
        "        print(f\"Found {len(csv_files)} CSV files: {[os.path.basename(f) for f in csv_files]}\")\n",
        "\n",
        "        dfs = []\n",
        "        for file in csv_files:\n",
        "            try:\n",
        "                print(f\"Loading {os.path.basename(file)}...\")\n",
        "                temp_df = pd.read_csv(file)\n",
        "                dfs.append(temp_df)\n",
        "                print(f\"  Shape: {temp_df.shape}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error loading {file}: {e}\")\n",
        "\n",
        "        print(\"Concatenating dataframes...\")\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "        print(f\"Combined data shape: {df.shape}\")\n",
        "        print(f\"Columns: {df.columns.tolist()}\")\n",
        "        print(df.head())\n",
        "\n",
        "        if df.duplicated().sum() > 0:\n",
        "            print(f\"Removing {df.duplicated().sum()} duplicate rows\")\n",
        "            df = df.drop_duplicates()\n",
        "\n",
        "        print(\"Looking for date/time columns...\")\n",
        "        datetime_cols = [col for col in df.columns if any(time_kw in col.lower() for time_kw in ['date', 'time', 'hour', 'day'])]\n",
        "\n",
        "        if datetime_cols:\n",
        "            print(f\"Found datetime columns: {datetime_cols}\")\n",
        "            if len(datetime_cols) == 1:\n",
        "                df['timestamp'] = pd.to_datetime(df[datetime_cols[0]])\n",
        "            else:\n",
        "                try:\n",
        "                    date_col = next(col for col in datetime_cols if 'date' in col.lower())\n",
        "                    time_col = next(col for col in datetime_cols if 'time' in col.lower() or 'hour' in col.lower())\n",
        "                    df['timestamp'] = pd.to_datetime(df[date_col] + ' ' + df[time_col])\n",
        "                except:\n",
        "                    print(\"Couldn't combine date and time columns\")\n",
        "                    df['timestamp'] = pd.to_datetime(df[datetime_cols[0]])\n",
        "\n",
        "            print(\"Sorting data by timestamp...\")\n",
        "            df = df.sort_values('timestamp')\n",
        "\n",
        "        print(\"Handling missing values...\")\n",
        "        missing_stats = df.isnull().sum()\n",
        "        print(f\"Missing values per column:\\n{missing_stats[missing_stats > 0]}\")\n",
        "\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        categorical_cols = [col for col in df.columns if col not in numeric_cols]\n",
        "        print(f\"Numeric columns: {len(numeric_cols)}\")\n",
        "        print(f\"Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if df[col].isnull().sum() > 0:\n",
        "                print(f\"  Filling NaN in {col} with median\")\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        df = df.dropna(subset=numeric_cols)\n",
        "\n",
        "        target_options = ['PM10', 'PM2.5', 'PM25', 'PM2_5', 'NO2', 'SO2', 'O3', 'CO']\n",
        "        available_targets = [col for col in df.columns if any(target in col for target in target_options)]\n",
        "\n",
        "        if not available_targets:\n",
        "            print(\"No standard air quality target found. Using last numeric column.\")\n",
        "            target_column = numeric_cols[-1]\n",
        "        else:\n",
        "            target_column = available_targets[0]\n",
        "\n",
        "        print(f\"Selected target column: {target_column}\")\n",
        "\n",
        "        exclude_cols = [target_column] + categorical_cols\n",
        "\n",
        "        if 'timestamp' in df.columns:\n",
        "            exclude_cols.append('timestamp')\n",
        "\n",
        "        for col in df.columns:\n",
        "            if any(id_kw in col.lower() for id_kw in ['id', 'station', 'code']):\n",
        "                exclude_cols.append(col)\n",
        "\n",
        "        feature_columns = [col for col in numeric_cols if col not in exclude_cols]\n",
        "        print(f\"Using {len(feature_columns)} feature columns: {feature_columns}\")\n",
        "\n",
        "        X = df[feature_columns].values\n",
        "        y = df[target_column].values.reshape(-1, 1)\n",
        "\n",
        "        X_scaler = StandardScaler()\n",
        "        y_scaler = StandardScaler()\n",
        "\n",
        "        X_scaled = X_scaler.fit_transform(X)\n",
        "        y_scaled = y_scaler.fit_transform(y)\n",
        "\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "            X_scaled, y_scaled, test_size=0.15, random_state=42\n",
        "        )\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_val, y_train_val, test_size=0.15/0.85, random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"Training set size: {X_train.shape[0]}\")\n",
        "        print(f\"Validation set size: {X_val.shape[0]}\")\n",
        "        print(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "        scalers = {'X': X_scaler, 'y': y_scaler}\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test, scalers, target_column\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "\n",
        "        print(\"Creating synthetic data for demonstration...\")\n",
        "\n",
        "        num_samples = 10000\n",
        "        num_features = 10\n",
        "\n",
        "        X = np.random.randn(num_samples, num_features)\n",
        "\n",
        "        y = 0.5 * X[:, 0] + 0.3 * X[:, 1] - 0.2 * X[:, 2] + 0.1 * np.random.randn(num_samples)\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "        X_scaler = StandardScaler()\n",
        "        y_scaler = StandardScaler()\n",
        "\n",
        "        X_scaled = X_scaler.fit_transform(X)\n",
        "        y_scaled = y_scaler.fit_transform(y)\n",
        "\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "            X_scaled, y_scaled, test_size=0.15, random_state=42\n",
        "        )\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_val, y_train_val, test_size=0.15/0.85, random_state=42\n",
        "        )\n",
        "\n",
        "        scalers = {'X': X_scaler, 'y': y_scaler}\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test, scalers, \"synthetic_target\"\n",
        "\n",
        "class BiLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        output, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        output = output[:, -1, :]\n",
        "\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        output, _ = self.rnn(x, h0)\n",
        "\n",
        "        output = output[:, -1, :]\n",
        "\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        self.positional_encoding = nn.Parameter(\n",
        "            torch.zeros(1, SEQUENCE_LENGTH, hidden_size)\n",
        "        )\n",
        "\n",
        "        nhead = 4\n",
        "        if hidden_size % nhead != 0:\n",
        "            nhead = 2\n",
        "            if hidden_size % nhead != 0:\n",
        "                nhead = 1\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=hidden_size * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layers,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = x + self.positional_encoding\n",
        "\n",
        "        output = self.transformer_encoder(x)\n",
        "\n",
        "        output = output[:, -1, :]\n",
        "\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_features, batch_targets in train_loader:\n",
        "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_features.size(0)\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_features, batch_targets in val_loader:\n",
        "                batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "                outputs = model(batch_features)\n",
        "                loss = criterion(outputs, batch_targets)\n",
        "\n",
        "                val_loss += loss.item() * batch_features.size(0)\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    history = {\n",
        "        'train_loss': train_losses,\n",
        "        'val_loss': val_losses\n",
        "    }\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, y_scaler=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_targets in test_loader:\n",
        "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_targets)\n",
        "\n",
        "            test_loss += loss.item() * batch_features.size(0)\n",
        "\n",
        "            predictions.append(outputs.cpu().numpy())\n",
        "            actuals.append(batch_targets.cpu().numpy())\n",
        "\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    actuals = np.concatenate(actuals)\n",
        "\n",
        "    if y_scaler:\n",
        "        predictions = y_scaler.inverse_transform(predictions)\n",
        "        actuals = y_scaler.inverse_transform(actuals)\n",
        "\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    r2 = r2_score(actuals, predictions)\n",
        "\n",
        "    metrics = {\n",
        "        'test_loss': test_loss,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'r2': r2,\n",
        "        'predictions': predictions,\n",
        "        'actuals': actuals\n",
        "    }\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'MSE: {mse:.4f}')\n",
        "    print(f'RMSE: {rmse:.4f}')\n",
        "    print(f'MAE: {mae:.4f}')\n",
        "    print(f'R²: {r2:.4f}')\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def plot_loss_curves(histories, model_names):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for i, history in enumerate(histories):\n",
        "        plt.plot(history['train_loss'], label=model_names[i])\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i, history in enumerate(histories):\n",
        "        plt.plot(history['val_loss'], label=model_names[i])\n",
        "    plt.title('Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('loss_curves.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_predictions(metrics_list, model_names, target_name):\n",
        "    num_models = len(metrics_list)\n",
        "    fig, axs = plt.subplots(1, num_models, figsize=(15, 5))\n",
        "\n",
        "    for i, metrics in enumerate(metrics_list):\n",
        "        ax = axs[i] if num_models > 1 else axs\n",
        "        ax.scatter(metrics['actuals'], metrics['predictions'], alpha=0.5)\n",
        "        ax.plot([np.min(metrics['actuals']), np.max(metrics['actuals'])],\n",
        "                [np.min(metrics['actuals']), np.max(metrics['actuals'])],\n",
        "                'r--')\n",
        "        ax.set_title(f'{model_names[i]}')\n",
        "        ax.set_xlabel(f'Actual {target_name}')\n",
        "        ax.set_ylabel(f'Predicted {target_name}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('predictions.png')\n",
        "    plt.show()\n",
        "\n",
        "def compare_metrics(metrics_list, model_names, target_name):\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Model': model_names,\n",
        "        'Test Loss': [metrics['test_loss'] for metrics in metrics_list],\n",
        "        'MSE': [metrics['mse'] for metrics in metrics_list],\n",
        "        'RMSE': [metrics['rmse'] for metrics in metrics_list],\n",
        "        'MAE': [metrics['mae'] for metrics in metrics_list],\n",
        "        'R²': [metrics['r2'] for metrics in metrics_list]\n",
        "    })\n",
        "\n",
        "    print(f\"\\nModel Comparison for {target_name} Prediction:\")\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    metrics_to_plot = ['Test Loss', 'MSE', 'RMSE', 'MAE']\n",
        "    num_metrics = len(metrics_to_plot)\n",
        "\n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        plt.bar(model_names, metrics_df[metric])\n",
        "        plt.title(f'{metric} for {target_name}')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('metrics_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(model_names, metrics_df['R²'])\n",
        "    plt.title(f'R² Score for {target_name} (higher is better)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('r2_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "print(\"Loading and preprocessing Seoul Air Quality data...\")\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, scalers, target_name = load_and_preprocess_seoul_data(DATA_PATH)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "\n",
        "print(f\"Input size: {input_size}\")\n",
        "print(f\"Output size: {output_size}\")\n",
        "print(f\"Target variable: {target_name}\")\n",
        "\n",
        "train_dataset = AirQualityDataset(X_train, y_train)\n",
        "val_dataset = AirQualityDataset(X_val, y_val)\n",
        "test_dataset = AirQualityDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "bilstm_model = BiLSTMModel(\n",
        "    input_size=input_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_size=output_size,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "rnn_model = RNNModel(\n",
        "    input_size=input_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_size=output_size,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "transformer_model = TransformerModel(\n",
        "    input_size=input_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_size=output_size,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "bilstm_optimizer = optim.Adam(bilstm_model.parameters(), lr=LEARNING_RATE)\n",
        "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)\n",
        "transformer_optimizer = optim.Adam(transformer_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"\\nTraining Bi-LSTM model...\")\n",
        "bilstm_model, bilstm_history = train_model(\n",
        "    bilstm_model, train_loader, val_loader, criterion, bilstm_optimizer, NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(\"\\nTraining RNN model...\")\n",
        "rnn_model, rnn_history = train_model(\n",
        "    rnn_model, train_loader, val_loader, criterion, rnn_optimizer, NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Transformer model...\")\n",
        "transformer_model, transformer_history = train_model(\n",
        "    transformer_model, train_loader, val_loader, criterion, transformer_optimizer, NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluating Bi-LSTM model...\")\n",
        "bilstm_metrics = evaluate_model(bilstm_model, test_loader, criterion, scalers['y'])\n",
        "\n",
        "print(\"\\nEvaluating RNN model...\")\n",
        "rnn_metrics = evaluate_model(rnn_model, test_loader, criterion, scalers['y'])\n",
        "\n",
        "print(\"\\nEvaluating Transformer model...\")\n",
        "transformer_metrics = evaluate_model(transformer_model, test_loader, criterion, scalers['y'])\n",
        "\n",
        "histories = [bilstm_history, rnn_history, transformer_history]\n",
        "model_names = ['Bi-LSTM', 'RNN', 'Transformer']\n",
        "plot_loss_curves(histories, model_names)\n",
        "\n",
        "metrics_list = [bilstm_metrics, rnn_metrics, transformer_metrics]\n",
        "plot_predictions(metrics_list, model_names, target_name)\n",
        "\n",
        "metrics_df = compare_metrics(metrics_list, model_names, target_name)\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/air_quality_models\"\n",
        "try:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"\\nSaving models to {output_dir}\")\n",
        "\n",
        "    torch.save(bilstm_model.state_dict(), f'{output_dir}/bilstm_model_seoul.pth')\n",
        "    torch.save(rnn_model.state_dict(), f'{output_dir}/rnn_model_seoul.pth')\n",
        "    torch.save(transformer_model.state_dict(), f'{output_dir}/transformer_model_seoul.pth')\n",
        "\n",
        "    print(\"Models saved successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving models: {e}\")\n",
        "    print(\"Saving models to current directory instead\")\n",
        "\n",
        "    torch.save(bilstm_model.state_dict(), 'bilstm_model_seoul.pth')\n",
        "    torch.save(rnn_model.state_dict(), 'rnn_model_seoul.pth')\n",
        "    torch.save(transformer_model.state_dict(), 'transformer_model_seoul.pth')\n",
        "\n",
        "print(\"\\nTraining and evaluation completed!\")\n",
        "print(\"Loss curves saved as 'loss_curves.png'\")\n",
        "print(\"Predictions plots saved as 'predictions.png'\")\n",
        "print(\"Metrics comparison saved as 'metrics_comparison.png' and 'r2_comparison.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Beijing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7Ct7-kAtVoy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import os\n",
        "import glob\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/data/beijing\"\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 50\n",
        "SEQUENCE_LENGTH = 24  # 24 hours sequence\n",
        "HIDDEN_SIZE = 64\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "\n",
        "class AirQualityDataset(Dataset):\n",
        "    def __init__(self, features, targets, seq_length=SEQUENCE_LENGTH):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx:idx + self.seq_length]\n",
        "        y = self.targets[idx + self.seq_length]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "def load_and_preprocess_beijing_data(data_dir):\n",
        "    try:\n",
        "        print(f\"Looking for CSV files in {data_dir}\")\n",
        "        csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n",
        "\n",
        "        if not csv_files:\n",
        "            print(f\"No CSV files found directly in {data_dir}, looking in subdirectories...\")\n",
        "            csv_files = glob.glob(os.path.join(data_dir, '**/*.csv'), recursive=True)\n",
        "\n",
        "        if not csv_files:\n",
        "            raise FileNotFoundError(f\"No CSV files found in {data_dir} or its subdirectories\")\n",
        "\n",
        "        print(f\"Found {len(csv_files)} CSV files: {[os.path.basename(f) for f in csv_files]}\")\n",
        "\n",
        "        # Beijing dataset often has specific formatting issues, so we need a more robust loading approach\n",
        "        all_dfs = []\n",
        "        for file in csv_files:\n",
        "            try:\n",
        "                print(f\"Loading {os.path.basename(file)}...\")\n",
        "\n",
        "                # Try different encodings if needed\n",
        "                try:\n",
        "                    df = pd.read_csv(file)\n",
        "                except UnicodeDecodeError:\n",
        "                    print(f\"  Retrying with different encoding for {file}\")\n",
        "                    df = pd.read_csv(file, encoding='latin1')\n",
        "\n",
        "                # Beijing data typically has a station name or number in the filename\n",
        "                station_name = os.path.basename(file).split('.')[0]\n",
        "                if 'station' not in df.columns:\n",
        "                    df['station'] = station_name\n",
        "\n",
        "                print(f\"  Shape: {df.shape}\")\n",
        "                print(f\"  Columns: {df.columns.tolist()[:5]}...\")\n",
        "                all_dfs.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"  Error loading {file}: {e}\")\n",
        "\n",
        "        print(f\"Successfully loaded {len(all_dfs)} CSV files\")\n",
        "\n",
        "        if not all_dfs:\n",
        "            raise ValueError(\"No data could be loaded from CSV files\")\n",
        "\n",
        "        print(\"Concatenating dataframes...\")\n",
        "        df = pd.concat(all_dfs, ignore_index=True)\n",
        "        print(f\"Combined data shape: {df.shape}\")\n",
        "\n",
        "        # Beijing dataset typically has year, month, day, hour columns instead of a single timestamp\n",
        "        print(\"Creating timestamp from date components...\")\n",
        "        time_components = []\n",
        "        for col in ['year', 'month', 'day', 'hour']:\n",
        "            if col in df.columns:\n",
        "                time_components.append(col)\n",
        "\n",
        "        if len(time_components) >= 3:  # We need at least year, month, day\n",
        "            if 'hour' not in time_components:\n",
        "                df['hour'] = 0  # Default to midnight if hour not provided\n",
        "\n",
        "            # Convert columns to string and ensure two digits for month, day, hour\n",
        "            for col in ['month', 'day', 'hour']:\n",
        "                if col in df.columns:\n",
        "                    df[col] = df[col].astype(str).str.zfill(2)\n",
        "\n",
        "            # Combine date parts into a timestamp\n",
        "            if 'hour' in df.columns:\n",
        "                df['timestamp'] = pd.to_datetime(\n",
        "                    df['year'].astype(str) + '-' +\n",
        "                    df['month'].astype(str) + '-' +\n",
        "                    df['day'].astype(str) + ' ' +\n",
        "                    df['hour'].astype(str) + ':00:00'\n",
        "                )\n",
        "            else:\n",
        "                df['timestamp'] = pd.to_datetime(\n",
        "                    df['year'].astype(str) + '-' +\n",
        "                    df['month'].astype(str) + '-' +\n",
        "                    df['day'].astype(str)\n",
        "                )\n",
        "        elif 'date' in df.columns and 'time' in df.columns:\n",
        "            df['timestamp'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
        "        elif 'date' in df.columns:\n",
        "            df['timestamp'] = pd.to_datetime(df['date'])\n",
        "        else:\n",
        "            print(\"Could not create timestamp from columns. Looking for existing timestamp column...\")\n",
        "            timestamp_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]\n",
        "            if timestamp_cols:\n",
        "                print(f\"Using {timestamp_cols[0]} as timestamp\")\n",
        "                df['timestamp'] = pd.to_datetime(df[timestamp_cols[0]])\n",
        "            else:\n",
        "                print(\"No timestamp column found. Data will not be sorted chronologically.\")\n",
        "\n",
        "        if 'timestamp' in df.columns:\n",
        "            print(\"Sorting data by timestamp...\")\n",
        "            df = df.sort_values('timestamp')\n",
        "\n",
        "        # Identify air quality target variables (common in Beijing dataset)\n",
        "        print(\"Identifying potential target variables...\")\n",
        "        target_options = ['PM2.5', 'PM10', 'NO2', 'CO', 'O3', 'SO2', 'PM25', 'PM_25', 'PM_10']\n",
        "\n",
        "        # Handle Beijing dataset column naming variations\n",
        "        beijing_columns = df.columns.tolist()\n",
        "        available_targets = []\n",
        "\n",
        "        for target in target_options:\n",
        "            matches = [col for col in beijing_columns if (\n",
        "                target.lower() in col.lower() or\n",
        "                target.replace('.', '_').lower() in col.lower() or\n",
        "                target.replace(' ', '').lower() in col.lower()\n",
        "            )]\n",
        "            available_targets.extend(matches)\n",
        "\n",
        "        if not available_targets:\n",
        "            print(\"No standard air quality target columns found.\")\n",
        "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "            target_column = numeric_cols[-1]\n",
        "        else:\n",
        "            # Prioritize PM2.5 if available, otherwise use the first available target\n",
        "            pm25_cols = [col for col in available_targets if 'pm2.5' in col.lower() or 'pm25' in col.lower() or 'pm_25' in col.lower()]\n",
        "            target_column = pm25_cols[0] if pm25_cols else available_targets[0]\n",
        "\n",
        "        print(f\"Selected target column: {target_column}\")\n",
        "\n",
        "        # Handle missing values\n",
        "        print(\"Handling missing values...\")\n",
        "        missing_stats = df.isnull().sum()\n",
        "        print(f\"Missing values per column:\\n{missing_stats[missing_stats > 0]}\")\n",
        "\n",
        "        # Beijing data often uses specific values for missing data\n",
        "        for col in df.columns:\n",
        "            # Replace common missing value indicators\n",
        "            if df[col].dtype in [np.float64, np.int64]:\n",
        "                # Some datasets use -999, -9999, or very large negative values to indicate missing data\n",
        "                df[col] = df[col].replace([-999, -9999], np.nan)\n",
        "                # Also replace unreasonably large or small values\n",
        "                if col != 'year':  # Don't filter year values\n",
        "                    df.loc[df[col] > 9999, col] = np.nan\n",
        "                    df.loc[df[col] < -9999, col] = np.nan\n",
        "\n",
        "        # Identify numeric and non-numeric columns\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        categorical_cols = [col for col in df.columns if col not in numeric_cols]\n",
        "        print(f\"Numeric columns: {len(numeric_cols)}\")\n",
        "        print(f\"Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "        # Fill missing values in numeric columns with median\n",
        "        for col in numeric_cols:\n",
        "            if df[col].isnull().sum() > 0:\n",
        "                print(f\"  Filling NaN in {col} with median\")\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        # Drop any remaining rows with NaN in numeric columns\n",
        "        df = df.dropna(subset=numeric_cols)\n",
        "\n",
        "        # Exclude non-feature columns from the feature set\n",
        "        exclude_cols = [target_column] + categorical_cols\n",
        "\n",
        "        # Also exclude date-related columns and station identifiers\n",
        "        exclude_patterns = ['year', 'month', 'day', 'hour', 'date', 'time', 'timestamp', 'station', 'lat', 'lon', 'latitude', 'longitude']\n",
        "        for col in df.columns:\n",
        "            if any(pattern in col.lower() for pattern in exclude_patterns):\n",
        "                if col not in exclude_cols:\n",
        "                    exclude_cols.append(col)\n",
        "\n",
        "        # Get the final feature columns\n",
        "        feature_columns = [col for col in numeric_cols if col not in exclude_cols]\n",
        "        print(f\"Using {len(feature_columns)} feature columns: {feature_columns}\")\n",
        "\n",
        "        # Check if we have enough features\n",
        "        if len(feature_columns) < 2:\n",
        "            print(\"Warning: Very few feature columns available. Adding additional features...\")\n",
        "            # If we don't have enough features, we can use lagged versions of the target as features\n",
        "            if 'timestamp' in df.columns and len(df) > 24:\n",
        "                print(\"Creating lag features of the target variable...\")\n",
        "                for lag in range(1, 13):  # Create 12 lag features\n",
        "                    lag_col = f\"{target_column}_lag_{lag}\"\n",
        "                    df[lag_col] = df[target_column].shift(lag)\n",
        "                    feature_columns.append(lag_col)\n",
        "\n",
        "                # Drop rows with NaN from lag creation\n",
        "                df = df.dropna()\n",
        "\n",
        "        # Handle extremely large datasets by sampling if needed\n",
        "        if len(df) > 100000:\n",
        "            print(f\"Dataset is very large ({len(df)} rows). Sampling 100,000 rows...\")\n",
        "            df = df.sample(n=100000, random_state=42)\n",
        "\n",
        "        # Extract features and target\n",
        "        X = df[feature_columns].values\n",
        "        y = df[target_column].values.reshape(-1, 1)\n",
        "\n",
        "        print(f\"Final data shape - X: {X.shape}, y: {y.shape}\")\n",
        "\n",
        "        # Normalize the data\n",
        "        X_scaler = StandardScaler()\n",
        "        y_scaler = StandardScaler()\n",
        "\n",
        "        X_scaled = X_scaler.fit_transform(X)\n",
        "        y_scaled = y_scaler.fit_transform(y)\n",
        "\n",
        "        # Split the data\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "            X_scaled, y_scaled, test_size=0.15, random_state=42\n",
        "        )\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_val, y_train_val, test_size=0.15/0.85, random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"Training set size: {X_train.shape[0]}\")\n",
        "        print(f\"Validation set size: {X_val.shape[0]}\")\n",
        "        print(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "        scalers = {'X': X_scaler, 'y': y_scaler}\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test, scalers, target_column\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "\n",
        "        print(\"Creating synthetic data for demonstration...\")\n",
        "\n",
        "        num_samples = 10000\n",
        "        num_features = 10\n",
        "\n",
        "        X = np.random.randn(num_samples, num_features)\n",
        "\n",
        "        y = 0.5 * X[:, 0] + 0.3 * X[:, 1] - 0.2 * X[:, 2] + 0.1 * np.random.randn(num_samples)\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "        X_scaler = StandardScaler()\n",
        "        y_scaler = StandardScaler()\n",
        "\n",
        "        X_scaled = X_scaler.fit_transform(X)\n",
        "        y_scaled = y_scaler.fit_transform(y)\n",
        "\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "            X_scaled, y_scaled, test_size=0.15, random_state=42\n",
        "        )\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_val, y_train_val, test_size=0.15/0.85, random_state=42\n",
        "        )\n",
        "\n",
        "        scalers = {'X': X_scaler, 'y': y_scaler}\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test, scalers, \"synthetic_target\"\n",
        "\n",
        "class BiLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        output, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        output = output[:, -1, :]\n",
        "\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        output, _ = self.rnn(x, h0)\n",
        "\n",
        "        output = output[:, -1, :]\n",
        "\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        self.positional_encoding = nn.Parameter(\n",
        "            torch.zeros(1, SEQUENCE_LENGTH, hidden_size)\n",
        "        )\n",
        "\n",
        "        nhead = 4\n",
        "        if hidden_size % nhead != 0:\n",
        "            nhead = 2\n",
        "            if hidden_size % nhead != 0:\n",
        "                nhead = 1\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=hidden_size * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layers,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = x + self.positional_encoding\n",
        "\n",
        "        output = self.transformer_encoder(x)\n",
        "\n",
        "        output = output[:, -1, :]\n",
        "\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_features, batch_targets in train_loader:\n",
        "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_features.size(0)\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_features, batch_targets in val_loader:\n",
        "                batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "                outputs = model(batch_features)\n",
        "                loss = criterion(outputs, batch_targets)\n",
        "\n",
        "                val_loss += loss.item() * batch_features.size(0)\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    history = {\n",
        "        'train_loss': train_losses,\n",
        "        'val_loss': val_losses\n",
        "    }\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, y_scaler=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_targets in test_loader:\n",
        "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_targets)\n",
        "\n",
        "            test_loss += loss.item() * batch_features.size(0)\n",
        "\n",
        "            predictions.append(outputs.cpu().numpy())\n",
        "            actuals.append(batch_targets.cpu().numpy())\n",
        "\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    actuals = np.concatenate(actuals)\n",
        "\n",
        "    if y_scaler:\n",
        "        predictions = y_scaler.inverse_transform(predictions)\n",
        "        actuals = y_scaler.inverse_transform(actuals)\n",
        "\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    r2 = r2_score(actuals, predictions)\n",
        "\n",
        "    metrics = {\n",
        "        'test_loss': test_loss,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'r2': r2,\n",
        "        'predictions': predictions,\n",
        "        'actuals': actuals\n",
        "    }\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'MSE: {mse:.4f}')\n",
        "    print(f'RMSE: {rmse:.4f}')\n",
        "    print(f'MAE: {mae:.4f}')\n",
        "    print(f'R²: {r2:.4f}')\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def plot_loss_curves(histories, model_names):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for i, history in enumerate(histories):\n",
        "        plt.plot(history['train_loss'], label=model_names[i])\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i, history in enumerate(histories):\n",
        "        plt.plot(history['val_loss'], label=model_names[i])\n",
        "    plt.title('Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('loss_curves.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_predictions(metrics_list, model_names, target_name):\n",
        "    num_models = len(metrics_list)\n",
        "    fig, axs = plt.subplots(1, num_models, figsize=(15, 5))\n",
        "\n",
        "    for i, metrics in enumerate(metrics_list):\n",
        "        ax = axs[i] if num_models > 1 else axs\n",
        "        ax.scatter(metrics['actuals'], metrics['predictions'], alpha=0.5)\n",
        "        ax.plot([np.min(metrics['actuals']), np.max(metrics['actuals'])],\n",
        "                [np.min(metrics['actuals']), np.max(metrics['actuals'])],\n",
        "                'r--')\n",
        "        ax.set_title(f'{model_names[i]}')\n",
        "        ax.set_xlabel(f'Actual {target_name}')\n",
        "        ax.set_ylabel(f'Predicted {target_name}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('predictions.png')\n",
        "    plt.show()\n",
        "\n",
        "def compare_metrics(metrics_list, model_names, target_name):\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Model': model_names,\n",
        "        'Test Loss': [metrics['test_loss'] for metrics in metrics_list],\n",
        "        'MSE': [metrics['mse'] for metrics in metrics_list],\n",
        "        'RMSE': [metrics['rmse'] for metrics in metrics_list],\n",
        "        'MAE': [metrics['mae'] for metrics in metrics_list],\n",
        "        'R²': [metrics['r2'] for metrics in metrics_list]\n",
        "    })\n",
        "\n",
        "    print(f\"\\nModel Comparison for {target_name} Prediction:\")\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    metrics_to_plot = ['Test Loss', 'MSE', 'RMSE', 'MAE']\n",
        "    num_metrics = len(metrics_to_plot)\n",
        "\n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        plt.bar(model_names, metrics_df[metric])\n",
        "        plt.title(f'{metric} for {target_name}')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('metrics_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(model_names, metrics_df['R²'])\n",
        "    plt.title(f'R² Score for {target_name} (higher is better)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('r2_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "print(\"Loading and preprocessing Beijing Air Quality data...\")\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, scalers, target_name = load_and_preprocess_beijing_data(DATA_PATH)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "\n",
        "print(f\"Input size: {input_size}\")\n",
        "print(f\"Output size: {output_size}\")\n",
        "print(f\"Target variable: {target_name}\")\n",
        "\n",
        "train_dataset = AirQualityDataset(X_train, y_train)\n",
        "val_dataset = AirQualityDataset(X_val, y_val)\n",
        "test_dataset = AirQualityDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "bilstm_model = BiLSTMModel(\n",
        "    input_size=input_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_size=output_size,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "rnn_model = RNNModel(\n",
        "    input_size=input_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_size=output_size,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "transformer_model = TransformerModel(\n",
        "    input_size=input_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_size=output_size,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "bilstm_optimizer = optim.Adam(bilstm_model.parameters(), lr=LEARNING_RATE)\n",
        "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)\n",
        "transformer_optimizer = optim.Adam(transformer_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"\\nTraining Bi-LSTM model...\")\n",
        "bilstm_model, bilstm_history = train_model(\n",
        "    bilstm_model, train_loader, val_loader, criterion, bilstm_optimizer, NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(\"\\nTraining RNN model...\")\n",
        "rnn_model, rnn_history = train_model(\n",
        "    rnn_model, train_loader, val_loader, criterion, rnn_optimizer, NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Transformer model...\")\n",
        "transformer_model, transformer_history = train_model(\n",
        "    transformer_model, train_loader, val_loader, criterion, transformer_optimizer, NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluating Bi-LSTM model...\")\n",
        "bilstm_metrics = evaluate_model(bilstm_model, test_loader, criterion, scalers['y'])\n",
        "\n",
        "print(\"\\nEvaluating RNN model...\")\n",
        "rnn_metrics = evaluate_model(rnn_model, test_loader, criterion, scalers['y'])\n",
        "\n",
        "print(\"\\nEvaluating Transformer model...\")\n",
        "transformer_metrics = evaluate_model(transformer_model, test_loader, criterion, scalers['y'])\n",
        "\n",
        "histories = [bilstm_history, rnn_history, transformer_history]\n",
        "model_names = ['Bi-LSTM', 'RNN', 'Transformer']\n",
        "plot_loss_curves(histories, model_names)\n",
        "\n",
        "metrics_list = [bilstm_metrics, rnn_metrics, transformer_metrics]\n",
        "plot_predictions(metrics_list, model_names, target_name)\n",
        "\n",
        "metrics_df = compare_metrics(metrics_list, model_names, target_name)\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/air_quality_models\"\n",
        "try:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"\\nSaving models to {output_dir}\")\n",
        "\n",
        "    torch.save(bilstm_model.state_dict(), f'{output_dir}/bilstm_model_beijing.pth')\n",
        "    torch.save(rnn_model.state_dict(), f'{output_dir}/rnn_model_beijing.pth')\n",
        "    torch.save(transformer_model.state_dict(), f'{output_dir}/transformer_model_beijing.pth')\n",
        "\n",
        "    print(\"Models saved successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving models: {e}\")\n",
        "    print(\"Saving models to current directory instead\")\n",
        "\n",
        "    torch.save(bilstm_model.state_dict(), 'bilstm_model_beijing.pth')\n",
        "    torch.save(rnn_model.state_dict(), 'rnn_model_beijing.pth')\n",
        "    torch.save(transformer_model.state_dict(), 'transformer_model_beijing.pth')\n",
        "\n",
        "print(\"\\nTraining and evaluation completed!\")\n",
        "print(\"Loss curves saved as 'loss_curves.png'\")\n",
        "print(\"Predictions plots saved as 'predictions.png'\")\n",
        "print(\"Metrics comparison saved as 'metrics_comparison.png' and 'r2_comparison.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0VYsW5dzPny"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eulvMtGRv4u8"
      },
      "source": [
        "#### Dublin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HryWp0Pl38P",
        "outputId": "1352027e-67ed-4246-f645-20a91f856241"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/data/airview_dublincity_roaddata_ugm3.csv\"\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 50\n",
        "SEQUENCE_LENGTH = 24\n",
        "HIDDEN_SIZE = 64\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.3\n",
        "\n",
        "class AirQualityDataset(Dataset):\n",
        "    def __init__(self, features, targets, seq_length=SEQUENCE_LENGTH):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx:idx + self.seq_length]\n",
        "        y = self.targets[idx + self.seq_length]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "def load_csv_data(filepath):\n",
        "    print(f\"Loading CSV data from {filepath}\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, delimiter=None, engine='python')\n",
        "        print(\"Successfully loaded data with auto-detected delimiter\")\n",
        "        return df\n",
        "    except:\n",
        "        print(\"Trying to handle special delimiters or file format issues...\")\n",
        "\n",
        "        with open(filepath, 'r') as f:\n",
        "            first_line = f.readline().strip()\n",
        "\n",
        "        if ',' in first_line:\n",
        "            sep = ','\n",
        "        elif ';' in first_line:\n",
        "            sep = ';'\n",
        "        elif '\\t' in first_line:\n",
        "            sep = '\\t'\n",
        "        else:\n",
        "            sep = None\n",
        "\n",
        "        print(f\"Detected delimiter: {sep if sep else 'unknown'}\")\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath, sep=sep, engine='python', error_bad_lines=False)\n",
        "            print(\"Successfully loaded data with detected delimiter and skipping bad lines\")\n",
        "            return df\n",
        "        except:\n",
        "            print(\"Trying to load with maximum error handling...\")\n",
        "            try:\n",
        "                df = pd.read_csv(filepath, sep=sep, engine='python', error_bad_lines=False,\n",
        "                                 warn_bad_lines=False, on_bad_lines='skip')\n",
        "                print(\"Successfully loaded data with maximum error tolerance\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Failed to load CSV file: {str(e)}\")\n",
        "\n",
        "def process_data(df, target_col=None):\n",
        "    print(\"Processing data...\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    print(f\"Sample data:\\n{df.head()}\")\n",
        "\n",
        "    string_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    print(f\"String columns that will be handled: {string_cols}\")\n",
        "\n",
        "    geometry_cols = [col for col in string_cols if\n",
        "                    any(geo_kw in col.lower() for geo_kw in ['geom', 'shape', 'line', 'point', 'polygon'])]\n",
        "\n",
        "    drop_cols = geometry_cols\n",
        "\n",
        "    for col in string_cols:\n",
        "        if col not in drop_cols:\n",
        "            try:\n",
        "                sample = df[col].iloc[0]\n",
        "                if isinstance(sample, str) and ('LINESTRING' in sample or 'POINT' in sample or 'POLYGON' in sample):\n",
        "                    drop_cols.append(col)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    print(f\"Dropping geometry columns: {drop_cols}\")\n",
        "    df = df.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    print(f\"Numeric columns: {numeric_cols}\")\n",
        "\n",
        "    pollution_keywords = ['no2', 'pm25', 'pm2_5', 'pm2.5', 'pm10', 'co', 'co2', 'o3', 'bc', 'ufp']\n",
        "\n",
        "    if target_col is None or target_col not in numeric_cols:\n",
        "        potential_targets = [col for col in numeric_cols if\n",
        "                            any(kw in col.lower() for kw in pollution_keywords)]\n",
        "\n",
        "        if potential_targets:\n",
        "            target_col = potential_targets[0]\n",
        "            print(f\"Automatically selected target column: {target_col}\")\n",
        "        else:\n",
        "            target_col = numeric_cols[-1]\n",
        "            print(f\"No pollution column found, using last numeric column as target: {target_col}\")\n",
        "\n",
        "    feature_cols = [col for col in numeric_cols if col != target_col]\n",
        "\n",
        "    print(f\"Using {len(feature_cols)} feature columns and target: {target_col}\")\n",
        "\n",
        "    df = df.dropna(subset=[target_col] + feature_cols)\n",
        "\n",
        "    X = df[feature_cols].values\n",
        "    y = df[target_col].values.reshape(-1, 1)\n",
        "\n",
        "    X_scaler = StandardScaler()\n",
        "    y_scaler = StandardScaler()\n",
        "\n",
        "    X_scaled = X_scaler.fit_transform(X)\n",
        "    y_scaled = y_scaler.fit_transform(y)\n",
        "\n",
        "    train_idx = int(0.7 * len(X_scaled))\n",
        "    val_idx = int(0.85 * len(X_scaled))\n",
        "\n",
        "    X_train = X_scaled[:train_idx]\n",
        "    y_train = y_scaled[:train_idx]\n",
        "\n",
        "    X_val = X_scaled[train_idx:val_idx]\n",
        "    y_val = y_scaled[train_idx:val_idx]\n",
        "\n",
        "    X_test = X_scaled[val_idx:]\n",
        "    y_test = y_scaled[val_idx:]\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, {'X': X_scaler, 'y': y_scaler}, target_col\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        output, _ = self.lstm(x, (h0, c0))\n",
        "        output = output[:, -1, :]\n",
        "        output = self.layer_norm(output)\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        output, _ = self.gru(x, h0)\n",
        "        output = output[:, -1, :]\n",
        "        output = self.layer_norm(output)\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def huber_loss(y_pred, y_true, delta=1.0):\n",
        "    residual = torch.abs(y_pred - y_true)\n",
        "    condition = residual < delta\n",
        "    squared_loss = 0.5 * residual ** 2\n",
        "    linear_loss = delta * (residual - 0.5 * delta)\n",
        "\n",
        "    return torch.mean(torch.where(condition, squared_loss, linear_loss))\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, num_epochs=NUM_EPOCHS):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_features, batch_targets in train_loader:\n",
        "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "            outputs = model(batch_features)\n",
        "            loss = huber_loss(outputs, batch_targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_features.size(0)\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_features, batch_targets in val_loader:\n",
        "                batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "                outputs = model(batch_features)\n",
        "                loss = huber_loss(outputs, batch_targets)\n",
        "\n",
        "                val_loss += loss.item() * batch_features.size(0)\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    return model, {'train_loss': train_losses, 'val_loss': val_losses}\n",
        "\n",
        "def evaluate_model(model, test_loader, y_scaler=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_targets in test_loader:\n",
        "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "            outputs = model(batch_features)\n",
        "            loss = huber_loss(outputs, batch_targets)\n",
        "\n",
        "            test_loss += loss.item() * batch_features.size(0)\n",
        "\n",
        "            predictions.append(outputs.cpu().numpy())\n",
        "            actuals.append(batch_targets.cpu().numpy())\n",
        "\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    actuals = np.concatenate(actuals)\n",
        "\n",
        "    if y_scaler:\n",
        "        predictions = y_scaler.inverse_transform(predictions)\n",
        "        actuals = y_scaler.inverse_transform(actuals)\n",
        "\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    r2 = r2_score(actuals, predictions)\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'MSE: {mse:.4f}')\n",
        "    print(f'RMSE: {rmse:.4f}')\n",
        "    print(f'MAE: {mae:.4f}')\n",
        "    print(f'R²: {r2:.4f}')\n",
        "\n",
        "    return {'test_loss': test_loss, 'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2,\n",
        "            'predictions': predictions, 'actuals': actuals}\n",
        "\n",
        "def plot_comparison(lstm_metrics, gru_metrics, target_name):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.scatter(lstm_metrics['actuals'], lstm_metrics['predictions'], alpha=0.5)\n",
        "    plt.plot([np.min(lstm_metrics['actuals']), np.max(lstm_metrics['actuals'])],\n",
        "             [np.min(lstm_metrics['actuals']), np.max(lstm_metrics['actuals'])], 'r--')\n",
        "    plt.title('LSTM Predictions')\n",
        "    plt.xlabel(f'Actual {target_name}')\n",
        "    plt.ylabel(f'Predicted {target_name}')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.scatter(gru_metrics['actuals'], gru_metrics['predictions'], alpha=0.5)\n",
        "    plt.plot([np.min(gru_metrics['actuals']), np.max(gru_metrics['actuals'])],\n",
        "             [np.min(gru_metrics['actuals']), np.max(gru_metrics['actuals'])], 'r--')\n",
        "    plt.title('GRU Predictions')\n",
        "    plt.xlabel(f'Actual {target_name}')\n",
        "    plt.ylabel(f'Predicted {target_name}')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.hist(lstm_metrics['actuals'] - lstm_metrics['predictions'], bins=50, alpha=0.7)\n",
        "    plt.title('LSTM Error Distribution')\n",
        "    plt.xlabel('Error')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.hist(gru_metrics['actuals'] - gru_metrics['predictions'], bins=50, alpha=0.7)\n",
        "    plt.title('GRU Error Distribution')\n",
        "    plt.xlabel('Error')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Model': ['LSTM', 'GRU'],\n",
        "        'MSE': [lstm_metrics['mse'], gru_metrics['mse']],\n",
        "        'RMSE': [lstm_metrics['rmse'], gru_metrics['rmse']],\n",
        "        'MAE': [lstm_metrics['mae'], gru_metrics['mae']],\n",
        "        'R²': [lstm_metrics['r2'], gru_metrics['r2']]\n",
        "    })\n",
        "\n",
        "    print(\"\\nModel Comparison:\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    metrics = ['MSE', 'RMSE', 'MAE']\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, [lstm_metrics['mse'], lstm_metrics['rmse'], lstm_metrics['mae']],\n",
        "            width, label='LSTM')\n",
        "    plt.bar(x + width/2, [gru_metrics['mse'], gru_metrics['rmse'], gru_metrics['mae']],\n",
        "            width, label='GRU')\n",
        "\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Error Metrics Comparison')\n",
        "    plt.xticks(x, metrics)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('metrics_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "try:\n",
        "    print(\"Loading and processing Dublin Air Quality data...\")\n",
        "\n",
        "    df = load_csv_data(DATA_PATH)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, scalers, target_name = process_data(df)\n",
        "\n",
        "    if len(X_train) < SEQUENCE_LENGTH + 1:\n",
        "        print(f\"WARNING: Not enough data for sequence length {SEQUENCE_LENGTH}\")\n",
        "        SEQUENCE_LENGTH = max(1, len(X_train) // 4)\n",
        "        print(f\"Reducing sequence length to {SEQUENCE_LENGTH}\")\n",
        "\n",
        "    input_size = X_train.shape[1]\n",
        "    output_size = y_train.shape[1]\n",
        "\n",
        "    print(f\"Input size: {input_size}\")\n",
        "    print(f\"Output size: {output_size}\")\n",
        "    print(f\"Target variable: {target_name}\")\n",
        "    print(f\"Training samples: {len(X_train)}\")\n",
        "\n",
        "    train_dataset = AirQualityDataset(X_train, y_train, SEQUENCE_LENGTH)\n",
        "    val_dataset = AirQualityDataset(X_val, y_val, SEQUENCE_LENGTH)\n",
        "    test_dataset = AirQualityDataset(X_test, y_test, SEQUENCE_LENGTH)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    lstm_model = LSTMModel(\n",
        "        input_size=input_size,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        output_size=output_size,\n",
        "        dropout=DROPOUT\n",
        "    )\n",
        "\n",
        "    gru_model = GRUModel(\n",
        "        input_size=input_size,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        output_size=output_size,\n",
        "        dropout=DROPOUT\n",
        "    )\n",
        "\n",
        "    lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "    gru_optimizer = optim.Adam(gru_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "    print(\"\\nTraining LSTM model...\")\n",
        "    lstm_model, lstm_history = train_model(\n",
        "        lstm_model, train_loader, val_loader, lstm_optimizer, NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining GRU model...\")\n",
        "    gru_model, gru_history = train_model(\n",
        "        gru_model, train_loader, val_loader, gru_optimizer, NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    print(\"\\nEvaluating LSTM model...\")\n",
        "    lstm_metrics = evaluate_model(lstm_model, test_loader, scalers['y'])\n",
        "\n",
        "    print(\"\\nEvaluating GRU model...\")\n",
        "    gru_metrics = evaluate_model(gru_model, test_loader, scalers['y'])\n",
        "\n",
        "    plot_comparison(lstm_metrics, gru_metrics, target_name)\n",
        "\n",
        "    print(\"\\nSaving models...\")\n",
        "    torch.save(lstm_model.state_dict(), 'lstm_model_dublin.pth')\n",
        "    torch.save(gru_model.state_dict(), 'gru_model_dublin.pth')\n",
        "\n",
        "    print(\"\\nTraining and evaluation completed!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {str(e)}\")\n",
        "    print(\"\\nPlease check that your dataset is in the correct format and location.\")\n",
        "    print(\"The code will try to handle various formats but needs a valid CSV file.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
